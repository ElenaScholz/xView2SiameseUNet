{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "base_dir = Path(__file__).resolve().parent.parent\n",
    "config_path = base_dir / \"notebooks\" / \"00_config.yaml\"\n",
    "print(base_dir)\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging und Speicherorte aus der YAML-Datei laden\n",
    "tensorboard_logs = config[\"foldernames\"][\"tensorboard_logs\"]\n",
    "checkpoints = config[\"foldernames\"][\"checkpoints\"]\n",
    "logfiles = config[\"foldernames\"][\"logfiles\"]\n",
    "class_counts_file = config[\"foldernames\"][\"class_counts_file\"]\n",
    "sample_weights_file = config[\"foldernames\"][\"sample_weights_file\"]\n",
    "\n",
    "# Definiere Pfade für Experiment-Verzeichnis (kann auch je nach Inferenz-Setup angepasst werden)\n",
    "experiment_group = config[\"data\"][\"experiment_group\"]\n",
    "experiment_id = config[\"data\"][\"experiment_id\"]\n",
    "\n",
    "# Beispiel: Pfad für TensorBoard Logs\n",
    "tensorboard_dir = Path(f\"/dss/dsshome1/08/{USER}/{experiment_group}/tensorboard_logs/{experiment_id}\")\n",
    "tensorboard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Beispiel: Pfad für Checkpoints\n",
    "checkpoint_dir = Path(f\"/dss/dsshome1/08/{USER}/{experiment_group}/checkpoints\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Beispiel: Lade Checkpoint (falls notwendig)\n",
    "checkpoint_path = checkpoint_dir / f\"{experiment_group}_{experiment_id}_best_siamese_unet_state.pth\"\n",
    "if checkpoint_path.exists():\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Checkpoint geladen: {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Kein Checkpoint unter {checkpoint_path} gefunden.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwende Gerät: cuda\n",
      "Verwende 3 GPUs!\n",
      "Loaded raw state_dict from /dss/dsshome1/08/di97ren/xView2_Subset/checkpoints/003_best_siamese_unet_state.pth\n",
      "Checkpoint erfolgreich in DataParallel-Modell geladen.\n",
      "Starte Modell-Evaluierung...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluiere:   9%|▊         | 10/117 [00:18<02:39,  1.49s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.helperfunctions import get_data_folder\n",
    "from utils.dataset import xView2Dataset, collate_fn, transform, image_transform\n",
    "from model.siameseNetwork import SiameseUnet\n",
    "from utils.helperfunctions import find_best_checkpoint, load_checkpoint\n",
    "\n",
    "import os\n",
    "base_dir = Path(os.getcwd()).parent  # Gehe einen Ordner zurück vom aktuellen Arbeitsverzeichnis\n",
    "config_path = base_dir / \"notebooks\" / \"00_config.yaml\"\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Pfade einrichten\n",
    "USER = config[\"data\"][\"user\"]\n",
    "USER_HOME_PATH = Path(f\"/dss/dsshome1/08/{USER}\")\n",
    "EXPERIMENT_GROUP = config[\"data\"][\"experiment_group\"]\n",
    "EXPERIMENT_ID = config[\"data\"][\"experiment_id\"]\n",
    "CHECKPOINTS_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"checkpoints\"\n",
    "\n",
    "# Test-Daten Ordner abrufen\n",
    "DATA_ROOT, EVAL_ROOT, EVAL_IMG, EVAL_LABEL, EVAL_TARGET, EVAL_PNG_IMAGES = get_data_folder(\n",
    "    config[\"data\"][\"validation_name\"], \n",
    "    main_dataset=config[\"data\"][\"use_main_dataset\"]\n",
    ")\n",
    "\n",
    "# Test-Dataset erstellen\n",
    "test_dataset = xView2Dataset(\n",
    "    png_path=EVAL_PNG_IMAGES, \n",
    "    target_path=EVAL_TARGET, \n",
    "    transform=transform(), \n",
    "    image_transform=image_transform()\n",
    ")\n",
    "\n",
    "# Gerät festlegen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Verwende Gerät: {device}\")\n",
    "\n",
    "# Modell erstellen\n",
    "model = SiameseUnet(num_pre_classes=2, num_post_classes=6)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Verwende {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "best_checkpoint_path = find_best_checkpoint(CHECKPOINTS_DIR, EXPERIMENT_ID)\n",
    "# Besten Checkpoint laden\n",
    "model = load_checkpoint(model, best_checkpoint_path)\n",
    "#model.eval()\n",
    "\n",
    "# Dataloader erstellen\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_pre_preds = []\n",
    "    all_pre_true = []\n",
    "    all_post_preds = []\n",
    "    all_post_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluiere\"):\n",
    "            # Batch auspacken\n",
    "            pre_images, post_images, pre_labels, post_labels = batch\n",
    "\n",
    "            # Auf Gerät verschieben\n",
    "            pre_images = pre_images.to(device)\n",
    "            post_images = post_images.to(device)\n",
    "            pre_labels = pre_labels.to(device)\n",
    "            post_labels = post_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pre_images, post_images)\n",
    "\n",
    "            # Annahme: Das Modell gibt das fusionierte Ergebnis zurück\n",
    "            # → Splitte die Vorhersagen in pre/post\n",
    "            pre_output = outputs[:, :2, :, :]   # erste 2 Kanäle → pre\n",
    "            post_output = outputs[:, 2:, :, :]  # restliche 4 Kanäle → post\n",
    "\n",
    "            # Vorhersagen abrufen\n",
    "            pre_preds = torch.argmax(pre_output, dim=1)      # (B, H, W)\n",
    "            post_preds = torch.argmax(post_output, dim=1)\n",
    "\n",
    "            # Labels auf richtige Form bringen\n",
    "            pre_labels = pre_labels.squeeze(1).long()        # (B, 1, H, W) → (B, H, W)\n",
    "            post_labels = post_labels.squeeze(1).long()\n",
    "\n",
    "            # Flatten und sammeln\n",
    "            all_pre_preds.extend(pre_preds.view(-1).cpu().numpy())\n",
    "            all_pre_true.extend(pre_labels.view(-1).cpu().numpy())\n",
    "            all_post_preds.extend(post_preds.view(-1).cpu().numpy())\n",
    "            all_post_true.extend(post_labels.view(-1).cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'pre_preds': np.array(all_pre_preds),\n",
    "        'pre_true': np.array(all_pre_true),\n",
    "        'post_preds': np.array(all_post_preds),\n",
    "        'post_true': np.array(all_post_true)\n",
    "    }\n",
    "\n",
    "# Evaluierung durchführen\n",
    "print(\"Starte Modell-Evaluierung...\")\n",
    "results = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "# Kennzahlen berechnen\n",
    "pre_accuracy = accuracy_score(results['pre_true'], results['pre_preds'])\n",
    "pre_f1_weighted = f1_score(results['pre_true'], results['pre_preds'], average='weighted')\n",
    "pre_f1_macro = f1_score(results['pre_true'], results['pre_preds'], average='macro')\n",
    "pre_cm = confusion_matrix(results['pre_true'], results['pre_preds'])\n",
    "\n",
    "post_accuracy = accuracy_score(results['post_true'], results['post_preds'])\n",
    "post_f1_weighted = f1_score(results['post_true'], results['post_preds'], average='weighted')\n",
    "post_f1_macro = f1_score(results['post_true'], results['post_preds'], average='macro')\n",
    "post_cm = confusion_matrix(results['post_true'], results['post_preds'])\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(\"\\nPre-Disaster Performance:\")\n",
    "print(f\"Accuracy: {pre_accuracy:.4f}\")\n",
    "print(f\"F1 Score (weighted): {pre_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (macro): {pre_f1_macro:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pre_cm)\n",
    "\n",
    "print(\"\\nPost-Disaster Performance:\")\n",
    "print(f\"Accuracy: {post_accuracy:.4f}\")\n",
    "print(f\"F1 Score (weighted): {post_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (macro): {post_f1_macro:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(post_cm)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results_dir = USER_HOME_PATH / EXPERIMENT_GROUP / \"evaluation_results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "result_file = results_dir / f\"{EXPERIMENT_ID}_evaluation_results.txt\"\n",
    "\n",
    "with open(result_file, 'w') as f:\n",
    "    f.write(\"MODEL EVALUATION RESULTS\\n\")\n",
    "    f.write(\"=======================\\n\\n\")\n",
    "    \n",
    "    f.write(\"Pre-Disaster Performance:\\n\")\n",
    "    f.write(f\"Accuracy: {pre_accuracy:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (weighted): {pre_f1_weighted:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (macro): {pre_f1_macro:.4f}\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(pre_cm) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Post-Disaster Performance:\\n\")\n",
    "    f.write(f\"Accuracy: {post_accuracy:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (weighted): {post_f1_weighted:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (macro): {post_f1_macro:.4f}\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(post_cm) + \"\\n\")\n",
    "\n",
    "print(f\"\\nEvaluierungsergebnisse gespeichert in: {result_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

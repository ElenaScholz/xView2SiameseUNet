{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "base_dir = Path(__file__).resolve().parent.parent\n",
    "config_path = base_dir / \"notebooks\" / \"00_config.yaml\"\n",
    "print(base_dir)\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging und Speicherorte aus der YAML-Datei laden\n",
    "tensorboard_logs = config[\"foldernames\"][\"tensorboard_logs\"]\n",
    "checkpoints = config[\"foldernames\"][\"checkpoints\"]\n",
    "logfiles = config[\"foldernames\"][\"logfiles\"]\n",
    "class_counts_file = config[\"foldernames\"][\"class_counts_file\"]\n",
    "sample_weights_file = config[\"foldernames\"][\"sample_weights_file\"]\n",
    "\n",
    "# Definiere Pfade für Experiment-Verzeichnis (kann auch je nach Inferenz-Setup angepasst werden)\n",
    "experiment_group = config[\"data\"][\"experiment_group\"]\n",
    "experiment_id = config[\"data\"][\"experiment_id\"]\n",
    "\n",
    "# Beispiel: Pfad für TensorBoard Logs\n",
    "tensorboard_dir = Path(f\"/dss/dsshome1/08/{USER}/{experiment_group}/tensorboard_logs/{experiment_id}\")\n",
    "tensorboard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Beispiel: Pfad für Checkpoints\n",
    "checkpoint_dir = Path(f\"/dss/dsshome1/08/{USER}/{experiment_group}/checkpoints\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Beispiel: Lade Checkpoint (falls notwendig)\n",
    "checkpoint_path = checkpoint_dir / f\"{experiment_group}_{experiment_id}_best_siamese_unet_state.pth\"\n",
    "if checkpoint_path.exists():\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Checkpoint geladen: {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Kein Checkpoint unter {checkpoint_path} gefunden.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwende Gerät: cuda\n",
      "Verwende 3 GPUs!\n",
      "Loaded raw state_dict from /dss/dsshome1/08/di97ren/xView2_Subset/checkpoints/003_best_siamese_unet_state.pth\n",
      "Checkpoint erfolgreich in DataParallel-Modell geladen.\n",
      "Starte Modell-Evaluierung...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluiere:  79%|███████▉  | 93/117 [02:26<00:34,  1.43s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.helperfunctions import get_data_folder\n",
    "from utils.dataset import xView2Dataset, collate_fn, transform, image_transform\n",
    "from model.siameseNetwork import SiameseUnet\n",
    "from utils.helperfunctions import find_best_checkpoint, load_checkpoint\n",
    "\n",
    "import os\n",
    "base_dir = Path(os.getcwd()).parent  # Gehe einen Ordner zurück vom aktuellen Arbeitsverzeichnis\n",
    "config_path = base_dir / \"notebooks\" / \"00_config.yaml\"\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Pfade einrichten\n",
    "USER = config[\"data\"][\"user\"]\n",
    "USER_HOME_PATH = Path(f\"/dss/dsshome1/08/{USER}\")\n",
    "EXPERIMENT_GROUP = config[\"data\"][\"experiment_group\"]\n",
    "EXPERIMENT_ID = config[\"data\"][\"experiment_id\"]\n",
    "CHECKPOINTS_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"checkpoints\"\n",
    "\n",
    "# Test-Daten Ordner abrufen\n",
    "DATA_ROOT, EVAL_ROOT, EVAL_IMG, EVAL_LABEL, EVAL_TARGET, EVAL_PNG_IMAGES = get_data_folder(\n",
    "    config[\"data\"][\"validation_name\"], \n",
    "    main_dataset=config[\"data\"][\"use_main_dataset\"]\n",
    ")\n",
    "\n",
    "# Test-Dataset erstellen\n",
    "test_dataset = xView2Dataset(\n",
    "    png_path=EVAL_PNG_IMAGES, \n",
    "    target_path=EVAL_TARGET, \n",
    "    transform=transform(), \n",
    "    image_transform=image_transform()\n",
    ")\n",
    "\n",
    "# Gerät festlegen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Verwende Gerät: {device}\")\n",
    "\n",
    "# Modell erstellen\n",
    "model = SiameseUnet(num_pre_classes=2, num_post_classes=6)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Verwende {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "best_checkpoint_path = find_best_checkpoint(CHECKPOINTS_DIR, EXPERIMENT_ID)\n",
    "# Besten Checkpoint laden\n",
    "model = load_checkpoint(model, best_checkpoint_path)\n",
    "#model.eval()\n",
    "\n",
    "# Dataloader erstellen\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_pre_preds = []\n",
    "    all_pre_true = []\n",
    "    all_post_preds = []\n",
    "    all_post_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluiere\"):\n",
    "            # Batch auspacken\n",
    "            pre_images, post_images, pre_labels, post_labels = batch\n",
    "\n",
    "            # Auf Gerät verschieben\n",
    "            pre_images = pre_images.to(device)\n",
    "            post_images = post_images.to(device)\n",
    "            pre_labels = pre_labels.to(device)\n",
    "            post_labels = post_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pre_images, post_images)\n",
    "\n",
    "            # Annahme: Das Modell gibt das fusionierte Ergebnis zurück\n",
    "            # → Splitte die Vorhersagen in pre/post\n",
    "            pre_output = outputs[:, :2, :, :]   # erste 2 Kanäle → pre\n",
    "            post_output = outputs[:, 2:, :, :]  # restliche 4 Kanäle → post\n",
    "\n",
    "            # Vorhersagen abrufen\n",
    "            pre_preds = torch.argmax(pre_output, dim=1)      # (B, H, W)\n",
    "            post_preds = torch.argmax(post_output, dim=1)\n",
    "\n",
    "            # Labels auf richtige Form bringen\n",
    "            pre_labels = pre_labels.squeeze(1).long()        # (B, 1, H, W) → (B, H, W)\n",
    "            post_labels = post_labels.squeeze(1).long()\n",
    "\n",
    "            # Flatten und sammeln\n",
    "            all_pre_preds.extend(pre_preds.view(-1).cpu().numpy())\n",
    "            all_pre_true.extend(pre_labels.view(-1).cpu().numpy())\n",
    "            all_post_preds.extend(post_preds.view(-1).cpu().numpy())\n",
    "            all_post_true.extend(post_labels.view(-1).cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'pre_preds': np.array(all_pre_preds),\n",
    "        'pre_true': np.array(all_pre_true),\n",
    "        'post_preds': np.array(all_post_preds),\n",
    "        'post_true': np.array(all_post_true)\n",
    "    }\n",
    "\n",
    "# Evaluierung durchführen\n",
    "print(\"Starte Modell-Evaluierung...\")\n",
    "results = evaluate_model(model, test_dataloader, device)\n",
    "print(\"pre_preds shape:\", results['pre_preds'].shape)\n",
    "print(\"pre_true shape:\", results['pre_true'].shape)\n",
    "print(\"unique pre_preds:\", np.unique(results['pre_preds']))\n",
    "print(\"unique pre_true:\", np.unique(results['pre_true']))\n",
    "# Kennzahlen berechnen\n",
    "pre_accuracy = accuracy_score(results['pre_true'], results['pre_preds'])\n",
    "pre_f1_weighted = f1_score(results['pre_true'], results['pre_preds'], average='weighted')\n",
    "pre_f1_macro = f1_score(results['pre_true'], results['pre_preds'], average='macro')\n",
    "pre_cm = confusion_matrix(results['pre_true'], results['pre_preds'])\n",
    "\n",
    "post_accuracy = accuracy_score(results['post_true'], results['post_preds'])\n",
    "post_f1_weighted = f1_score(results['post_true'], results['post_preds'], average='weighted')\n",
    "post_f1_macro = f1_score(results['post_true'], results['post_preds'], average='macro')\n",
    "post_cm = confusion_matrix(results['post_true'], results['post_preds'])\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(\"\\nPre-Disaster Performance:\")\n",
    "print(f\"Accuracy: {pre_accuracy:.4f}\")\n",
    "print(f\"F1 Score (weighted): {pre_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (macro): {pre_f1_macro:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pre_cm)\n",
    "\n",
    "print(\"\\nPost-Disaster Performance:\")\n",
    "print(f\"Accuracy: {post_accuracy:.4f}\")\n",
    "print(f\"F1 Score (weighted): {post_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (macro): {post_f1_macro:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(post_cm)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results_dir = USER_HOME_PATH / EXPERIMENT_GROUP / \"evaluation_results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "result_file = results_dir / f\"{EXPERIMENT_ID}_evaluation_results.txt\"\n",
    "\n",
    "with open(result_file, 'w') as f:\n",
    "    f.write(\"MODEL EVALUATION RESULTS\\n\")\n",
    "    f.write(\"=======================\\n\\n\")\n",
    "    \n",
    "    f.write(\"Pre-Disaster Performance:\\n\")\n",
    "    f.write(f\"Accuracy: {pre_accuracy:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (weighted): {pre_f1_weighted:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (macro): {pre_f1_macro:.4f}\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(pre_cm) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Post-Disaster Performance:\\n\")\n",
    "    f.write(f\"Accuracy: {post_accuracy:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (weighted): {post_f1_weighted:.4f}\\n\")\n",
    "    f.write(f\"F1 Score (macro): {post_f1_macro:.4f}\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(post_cm) + \"\\n\")\n",
    "\n",
    "print(f\"\\nEvaluierungsergebnisse gespeichert in: {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Konfiguration von: /dss/dsshome1/08/di97ren/04-geo-oma24/xView2SiameseUNet/notebooks/00_config.yaml\n",
      "Evaluationsergebnisse werden gespeichert in: /dss/dsshome1/08/di97ren/xView2_all_data/evaluation/001\n",
      "Verwende Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/08/di97ren/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell von: /dss/dsshome1/08/di97ren/xView2_all_data/checkpoints/001_best_siamese_unet_state.pth\n",
      "Starte Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/234 [02:03<7:57:43, 123.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 273\u001b[39m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMetriken-Zusammenfassung gespeichert als: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVALUATION_DIR\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmetrics_summary.png\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    158\u001b[39m y_post_metric = y_post.squeeze(\u001b[32m1\u001b[39m).long()\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_post\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Berechne Loss\u001b[39;00m\n\u001b[32m    164\u001b[39m loss = combined_loss_function(pred, y_pre_metric, y_post_metric, focal_loss_pre, focal_loss_post)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/notebooks/model/siameseNetwork.py:21\u001b[39m, in \u001b[36mSiameseUnet.forward\u001b[39m\u001b[34m(self, pre_image, post_image)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pre_image, post_image):\n\u001b[32m     20\u001b[39m     pre_output = \u001b[38;5;28mself\u001b[39m.unet_preDisaster(pre_image)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     post_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munet_postDisaster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Konkatenieren der Ausgaben\u001b[39;00m\n\u001b[32m     24\u001b[39m     fused_output = torch.cat([pre_output, post_output], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/notebooks/model/uNet.py:58\u001b[39m, in \u001b[36mUNet_ResNet50.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     54\u001b[39m original_size = x.shape[\u001b[32m2\u001b[39m:]\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Pass the input through encoder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# Pass the extracted features through the decoder to upsampple\u001b[39;00m\n\u001b[32m     61\u001b[39m     decoder_out = \u001b[38;5;28mself\u001b[39m.decoder(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py:146\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    144\u001b[39m     identity = x\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.bn1(out)\n\u001b[32m    148\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.relu(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/04-geo-oma24/xView2SiameseUNet/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score, MulticlassJaccardIndex\n",
    "\n",
    "from utils.helperfunctions import get_data_folder\n",
    "from utils.dataset import xView2Dataset, collate_fn, transform, image_transform\n",
    "from model.siameseNetwork import SiameseUnet\n",
    "from model.loss import FocalLoss, combined_loss_function\n",
    "\n",
    "def evaluate_model():\n",
    "    # Lade Konfiguration\n",
    "    base_dir = Path(os.getcwd()).parent  # Gehe einen Ordner zurück vom aktuellen Arbeitsverzeichnis\n",
    "    config_path = base_dir / \"notebooks\" / \"00_config.yaml\"\n",
    "    print(f\"Lade Konfiguration von: {config_path}\")\n",
    "\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # Datenpfade für Testset\n",
    "    DATA_ROOT, TEST_ROOT, TEST_IMG, TEST_LABEL, TEST_TARGET, TEST_PNG_IMAGES = get_data_folder(\n",
    "        config[\"data\"][\"test_name\"], \n",
    "        main_dataset=config[\"data\"][\"use_main_dataset\"]\n",
    "    )\n",
    "\n",
    "    USER = config[\"data\"][\"user\"]\n",
    "    USER_HOME_PATH = Path(f\"/dss/dsshome1/08/{USER}\")\n",
    "\n",
    "    # Verzeichnisse einrichten\n",
    "    EXPERIMENT_GROUP = config[\"data\"][\"experiment_group\"]\n",
    "    EXPERIMENT_ID = config[\"data\"][\"experiment_id\"]\n",
    "    CHECKPOINTS_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"checkpoints\"\n",
    "    EVALUATION_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"evaluation\" / EXPERIMENT_ID\n",
    "    EVALUATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Evaluationsergebnisse werden gespeichert in: {EVALUATION_DIR}\")\n",
    "\n",
    "    # Device Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Verwende Device: {device}\")\n",
    "\n",
    "    # Lade Klassenzählungen für die Loss-Berechnung\n",
    "    class_counts_path = os.path.join(base_dir, \"precalculations\", \"class_counts.json\")\n",
    "    with open(class_counts_path, 'r') as f:\n",
    "        class_counts = json.load(f)\n",
    "        # Anpassen an das tatsächliche Format der JSON-Datei\n",
    "        pre_counts = class_counts[\"pre\"]\n",
    "        post_counts = class_counts[\"post\"]\n",
    "\n",
    "    # Berechne Klassengewichte wie im Training\n",
    "    pre_weights = {}\n",
    "    post_weights = {}\n",
    "\n",
    "    # Konvertiere Strings zu Integers für die Zählungen\n",
    "    pre_counts = {int(k): int(v) for k, v in pre_counts.items()}\n",
    "    post_counts = {int(k): int(v) for k, v in post_counts.items()}\n",
    "\n",
    "    total_pre = sum(pre_counts.values())\n",
    "    for cls, count in pre_counts.items():\n",
    "        pre_weights[cls] = 1.0 / (count / total_pre) if count > 0 else 1.0\n",
    "\n",
    "    total_post = sum(post_counts.values())\n",
    "    for cls, count in post_counts.items():\n",
    "        post_weights[cls] = 1.0 / (count / total_post) if count > 0 else 1.0\n",
    "\n",
    "    # Konvertiere zu Tensoren\n",
    "    class_weights_pre = torch.tensor([\n",
    "        pre_weights.get(0, 1.0), \n",
    "        pre_weights.get(1, 10.0)\n",
    "    ], device=device)\n",
    "\n",
    "    class_weights_post = torch.tensor([\n",
    "        post_weights.get(0, 1.0), \n",
    "        post_weights.get(1, 10.0),\n",
    "        post_weights.get(2, 30.0),\n",
    "        post_weights.get(3, 20.0),\n",
    "        post_weights.get(4, 50.0),\n",
    "        post_weights.get(5, 100.0)\n",
    "    ], device=device)\n",
    "\n",
    "    # Initialisiere Loss-Funktionen\n",
    "    focal_loss_pre = FocalLoss(alpha=class_weights_pre, gamma=config[\"focal_loss\"][\"gamma\"])\n",
    "    focal_loss_post = FocalLoss(alpha=class_weights_post, gamma=config[\"focal_loss\"][\"gamma\"])\n",
    "\n",
    "    # Erstelle Test-Dataset\n",
    "    test_dataset = xView2Dataset(\n",
    "        png_path=TEST_PNG_IMAGES, \n",
    "        target_path=TEST_TARGET, \n",
    "        transform=transform(), \n",
    "        image_transform=image_transform()\n",
    "    )\n",
    "\n",
    "    # Konfiguriere Dataloader\n",
    "    if device.type == \"cuda\":\n",
    "        num_workers = torch.cuda.device_count() * config[\"dataloader\"][\"num_workers_multiplier\"]\n",
    "    else:\n",
    "        num_workers = 3\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config[\"training\"][\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=config[\"dataloader\"][\"pin_memory\"]\n",
    "    )\n",
    "\n",
    "    # Lade Modell\n",
    "    model = SiameseUnet(num_pre_classes=2, num_post_classes=6)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Verwende {torch.cuda.device_count()} GPUs!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Lade besten Checkpoint\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"{EXPERIMENT_ID}_best_siamese_unet_state.pth\"\n",
    "    print(f\"Lade Modell von: {checkpoint_path}\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Definiere Evaluationsmetriken\n",
    "    precision_pre = MulticlassPrecision(num_classes=2).to(device)\n",
    "    recall_pre = MulticlassRecall(num_classes=2).to(device)\n",
    "    f1_pre = MulticlassF1Score(num_classes=2).to(device)\n",
    "    iou_pre = MulticlassJaccardIndex(num_classes=2).to(device)\n",
    "\n",
    "    precision_post = MulticlassPrecision(num_classes=6).to(device)\n",
    "    recall_post = MulticlassRecall(num_classes=6).to(device)\n",
    "    f1_post = MulticlassF1Score(num_classes=6).to(device)\n",
    "    iou_post = MulticlassJaccardIndex(num_classes=6).to(device)\n",
    "\n",
    "    # Klassennamen für bessere Lesbarkeit\n",
    "    pre_class_names = [\"No Building\", \"Building\"]\n",
    "    post_class_names = [\"No Building\", \"Minor Damage\", \"Major Damage\", \"Destroyed\", \"Flooded\", \"Other Damage\"]\n",
    "\n",
    "    # Evaluiere das Modell\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    print(\"Starte Evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (pre_imgs, post_imgs, pre_masks, post_masks) in enumerate(tqdm(test_dataloader)):\n",
    "            X_pre = pre_imgs.to(device).float()\n",
    "            X_post = post_imgs.to(device).float()\n",
    "            y_pre = pre_masks.to(device)\n",
    "            y_post = post_masks.to(device)\n",
    "            \n",
    "            # Vorbereitung der Masken für die Metriken\n",
    "            y_pre_metric = y_pre.squeeze(1).long()\n",
    "            y_post_metric = y_post.squeeze(1).long()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(X_pre, X_post)\n",
    "            \n",
    "            # Berechne Loss\n",
    "            loss = combined_loss_function(pred, y_pre_metric, y_post_metric, focal_loss_pre, focal_loss_post)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Aktualisiere Metriken\n",
    "            precision_pre.update(pred[:, :2], y_pre_metric)\n",
    "            recall_pre.update(pred[:, :2], y_pre_metric)\n",
    "            f1_pre.update(pred[:, :2], y_pre_metric)\n",
    "            iou_pre.update(pred[:, :2], y_pre_metric)\n",
    "            \n",
    "            precision_post.update(pred[:, 2:], y_post_metric)\n",
    "            recall_post.update(pred[:, 2:], y_post_metric)\n",
    "            f1_post.update(pred[:, 2:], y_post_metric)\n",
    "            iou_post.update(pred[:, 2:], y_post_metric)\n",
    "            \n",
    "            # Speichere ein paar Beispielbilder für Visualisierung (optional)\n",
    "            if i == 0:\n",
    "                # Hier können Beispielbilder gespeichert werden (wird aus Gründen der Einfachheit weggelassen)\n",
    "                pass\n",
    "    \n",
    "    # Berechne durchschnittlichen Loss\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    \n",
    "    # Berechne finale Metriken\n",
    "    precision_pre_value = precision_pre.compute()\n",
    "    recall_pre_value = recall_pre.compute()\n",
    "    f1_pre_value = f1_pre.compute()\n",
    "    iou_pre_value = iou_pre.compute()\n",
    "    \n",
    "    precision_post_value = precision_post.compute()\n",
    "    recall_post_value = recall_post.compute()\n",
    "    f1_post_value = f1_post.compute()\n",
    "    iou_post_value = iou_post.compute()\n",
    "    \n",
    "    # Speichere Ergebnisse\n",
    "    with open(EVALUATION_DIR / \"evaluation_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Test Loss: {avg_test_loss:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Pre-Disaster Metriken:\\n\")\n",
    "        f.write(f\"Durchschnittliche Precision: {precision_pre_value.mean().item():.4f}\\n\")\n",
    "        f.write(f\"Durchschnittliche Recall: {recall_pre_value.mean().item():.4f}\\n\")\n",
    "        f.write(f\"Durchschnittlicher F1-Score: {f1_pre_value.mean().item():.4f}\\n\")\n",
    "        f.write(f\"Durchschnittlicher IoU: {iou_pre_value.mean().item():.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Post-Disaster Metriken:\\n\")\n",
    "        f.write(f\"Durchschnittliche Precision: {precision_post_value.mean().item():.4f}\\n\")\n",
    "        f.write(f\"Durchschnittliche Recall: {recall_post_value.mean().item():.4f}\\n\")\n",
    "        f.write(f\"Durchschnittlicher F1-Score: {f1_post_value.mean().item():.4f}\\n\")\n",
    "        f.write(f\"Durchschnittlicher IoU: {iou_post_value.mean().item():.4f}\\n\\n\")\n",
    "        \n",
    "        # Klassenspezifische Metriken (Pre-Disaster)\n",
    "        f.write(\"Klassenspezifische Metriken (Pre-Disaster):\\n\")\n",
    "        for i, class_name in enumerate(pre_class_names):\n",
    "            f.write(f\"Klasse {i} ({class_name}):\\n\")\n",
    "            f.write(f\"  Precision: {precision_pre_value[i].item():.4f}\\n\")\n",
    "            f.write(f\"  Recall: {recall_pre_value[i].item():.4f}\\n\")\n",
    "            f.write(f\"  F1-Score: {f1_pre_value[i].item():.4f}\\n\")\n",
    "            f.write(f\"  IoU: {iou_pre_value[i].item():.4f}\\n\\n\")\n",
    "        \n",
    "        # Klassenspezifische Metriken (Post-Disaster)\n",
    "        f.write(\"Klassenspezifische Metriken (Post-Disaster):\\n\")\n",
    "        for i, class_name in enumerate(post_class_names):\n",
    "            f.write(f\"Klasse {i} ({class_name}):\\n\")\n",
    "            f.write(f\"  Precision: {precision_post_value[i].item():.4f}\\n\")\n",
    "            f.write(f\"  Recall: {recall_post_value[i].item():.4f}\\n\")\n",
    "            f.write(f\"  F1-Score: {f1_post_value[i].item():.4f}\\n\")\n",
    "            f.write(f\"  IoU: {iou_post_value[i].item():.4f}\\n\\n\")\n",
    "    \n",
    "    # Drucke zusammenfassende Metriken\n",
    "    print(\"\\nEvaluationsergebnisse:\")\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(\"\\nPre-Disaster Metriken:\")\n",
    "    print(f\"Durchschnittliche Precision: {precision_pre_value.mean().item():.4f}\")\n",
    "    print(f\"Durchschnittliche Recall: {recall_pre_value.mean().item():.4f}\")\n",
    "    print(f\"Durchschnittlicher F1-Score: {f1_pre_value.mean().item():.4f}\")\n",
    "    print(f\"Durchschnittlicher IoU: {iou_pre_value.mean().item():.4f}\")\n",
    "    \n",
    "    print(\"\\nPost-Disaster Metriken:\")\n",
    "    print(f\"Durchschnittliche Precision: {precision_post_value.mean().item():.4f}\")\n",
    "    print(f\"Durchschnittliche Recall: {recall_post_value.mean().item():.4f}\")\n",
    "    print(f\"Durchschnittlicher F1-Score: {f1_post_value.mean().item():.4f}\")\n",
    "    print(f\"Durchschnittlicher IoU: {iou_post_value.mean().item():.4f}\")\n",
    "    \n",
    "    print(f\"\\nAusführliche Ergebnisse wurden gespeichert in: {EVALUATION_DIR / 'evaluation_results.txt'}\")\n",
    "    \n",
    "    # Optional: Erstelle einen einfachen Plot für die Visualisierung (kann erweitert werden)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'IoU']\n",
    "    pre_values = [precision_pre_value.mean().item(), recall_pre_value.mean().item(), \n",
    "                 f1_pre_value.mean().item(), iou_pre_value.mean().item()]\n",
    "    post_values = [precision_post_value.mean().item(), recall_post_value.mean().item(), \n",
    "                  f1_post_value.mean().item(), iou_post_value.mean().item()]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, pre_values, width, label='Pre-Disaster')\n",
    "    plt.bar(x + width/2, post_values, width, label='Post-Disaster')\n",
    "    \n",
    "    plt.xlabel('Metrik')\n",
    "    plt.ylabel('Wert')\n",
    "    plt.title('Zusammenfassung der Evaluationsmetriken')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(EVALUATION_DIR / \"metrics_summary.png\")\n",
    "    print(f\"Metriken-Zusammenfassung gespeichert als: {EVALUATION_DIR / 'metrics_summary.png'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load class weights for loss calculation\n",
    "class_counts_path = os.path.join(base_dir, \"precalculations\", \"class_counts.json\")\n",
    "with open(class_counts_path, 'r') as f:\n",
    "    class_counts = json.load(f)\n",
    "    pre_counts = class_counts[\"pre_counts\"]\n",
    "    post_counts = class_counts[\"post_counts\"]\n",
    "\n",
    "# Create class weights as in training\n",
    "pre_weights = {}\n",
    "post_weights = {}\n",
    "\n",
    "# Calculate class weights (same logic as in training)\n",
    "total_pre = sum(pre_counts.values())\n",
    "for cls, count in pre_counts.items():\n",
    "    pre_weights[int(cls)] = 1.0 / (count / total_pre) if count > 0 else 1.0\n",
    "\n",
    "total_post = sum(post_counts.values())\n",
    "for cls, count in post_counts.items():\n",
    "    post_weights[int(cls)] = 1.0 / (count / total_post) if count > 0 else 1.0\n",
    "\n",
    "# Convert to tensors\n",
    "class_weights_pre = torch.tensor([\n",
    "    pre_weights.get(0, 1.0), \n",
    "    pre_weights.get(1, 10.0)\n",
    "], device=device)\n",
    "\n",
    "class_weights_post = torch.tensor([\n",
    "    post_weights.get(0, 1.0), \n",
    "    post_weights.get(1, 10.0),\n",
    "    post_weights.get(2, 30.0),\n",
    "    post_weights.get(3, 20.0),\n",
    "    post_weights.get(4, 50.0),\n",
    "    post_weights.get(5, 100.0)\n",
    "], device=device)\n",
    "\n",
    "# Initialize loss functions\n",
    "focal_loss_pre = FocalLoss(alpha=class_weights_pre, gamma=config[\"focal_loss\"][\"gamma\"])\n",
    "focal_loss_post = FocalLoss(alpha=class_weights_post, gamma=config[\"focal_loss\"][\"gamma\"])\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = xView2Dataset(\n",
    "    png_path=TEST_PNG_IMAGES, \n",
    "    target_path=TEST_TARGET, \n",
    "    transform=transform(), \n",
    "    image_transform=image_transform()\n",
    ")\n",
    "\n",
    "# Configure dataloader\n",
    "if device == \"cuda\":\n",
    "    num_workers = torch.cuda.device_count() * config[\"dataloader\"][\"num_workers_multiplier\"]\n",
    "else:\n",
    "    num_workers = 4\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=config[\"dataloader\"][\"pin_memory\"]\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = SiameseUnet(num_pre_classes=2, num_post_classes=6)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint_path = CHECKPOINTS_DIR / f\"{EXPERIMENT_ID}_best_siamese_unet_state.pth\"\n",
    "print(f\"Loading model from: {checkpoint_path}\")\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define evaluation metrics\n",
    "precision_pre = MulticlassPrecision(num_classes=2).to(device)\n",
    "recall_pre = MulticlassRecall(num_classes=2).to(device)\n",
    "f1_pre = MulticlassF1Score(num_classes=2).to(device)\n",
    "iou_pre = MulticlassJaccardIndex(num_classes=2).to(device)\n",
    "\n",
    "precision_post = MulticlassPrecision(num_classes=6).to(device)\n",
    "recall_post = MulticlassRecall(num_classes=6).to(device)\n",
    "f1_post = MulticlassF1Score(num_classes=6).to(device)\n",
    "iou_post = MulticlassJaccardIndex(num_classes=6).to(device)\n",
    "\n",
    "# Class names for better readability\n",
    "pre_class_names = [\"No Building\", \"Building\"]\n",
    "post_class_names = [\"No Building\", \"Minor Damage\", \"Major Damage\", \"Destroyed\", \"Flooded\", \"Other Damage\"]\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"Evaluate the model on the test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_pre_preds = []\n",
    "    all_pre_targets = []\n",
    "    all_post_preds = []\n",
    "    all_post_targets = []\n",
    "    sample_images = []\n",
    "    sample_count = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (pre_imgs, post_imgs, pre_masks, post_masks) in enumerate(tqdm(test_dataloader)):\n",
    "            X_pre = pre_imgs.to(device).float()\n",
    "            X_post = post_imgs.to(device).float()\n",
    "            y_pre = pre_masks.to(device)\n",
    "            y_post = post_masks.to(device)\n",
    "            \n",
    "            # Prepare masks for metrics\n",
    "            y_pre_metric = y_pre.squeeze(1).long()\n",
    "            y_post_metric = y_post.squeeze(1).long()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(X_pre, X_post)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = combined_loss_function(pred, y_pre_metric, y_post_metric, focal_loss_pre, focal_loss_post)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            pre_pred = torch.argmax(pred[:, :2], dim=1)\n",
    "            post_pred = torch.argmax(pred[:, 2:], dim=1)\n",
    "            \n",
    "            # Store batch predictions and targets for overall metrics\n",
    "            all_pre_preds.append(pre_pred.cpu())\n",
    "            all_pre_targets.append(y_pre_metric.cpu())\n",
    "            all_post_preds.append(post_pred.cpu())\n",
    "            all_post_targets.append(y_post_metric.cpu())\n",
    "            \n",
    "            # Update metrics\n",
    "            precision_pre.update(pred[:, :2], y_pre_metric)\n",
    "            recall_pre.update(pred[:, :2], y_pre_metric)\n",
    "            f1_pre.update(pred[:, :2], y_pre_metric)\n",
    "            iou_pre.update(pred[:, :2], y_pre_metric)\n",
    "            \n",
    "            precision_post.update(pred[:, 2:], y_post_metric)\n",
    "            recall_post.update(pred[:, 2:], y_post_metric)\n",
    "            f1_post.update(pred[:, 2:], y_post_metric)\n",
    "            iou_post.update(pred[:, 2:], y_post_metric)\n",
    "            \n",
    "            # Save some sample images for visualization (first 5 batches)\n",
    "            if i < 5:\n",
    "                for j in range(min(2, len(pre_imgs))):  # Take 2 samples from each batch\n",
    "                    sample_images.append({\n",
    "                        'pre_img': pre_imgs[j].cpu().numpy(),\n",
    "                        'post_img': post_imgs[j].cpu().numpy(),\n",
    "                        'pre_mask': pre_masks[j].cpu().numpy(),\n",
    "                        'post_mask': post_masks[j].cpu().numpy(),\n",
    "                        'pre_pred': pre_pred[j].cpu().numpy(),\n",
    "                        'post_pred': post_pred[j].cpu().numpy()\n",
    "                    })\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    \n",
    "    # Compute final metrics\n",
    "    precision_pre_value = precision_pre.compute()\n",
    "    recall_pre_value = recall_pre.compute()\n",
    "    f1_pre_value = f1_pre.compute()\n",
    "    iou_pre_value = iou_pre.compute()\n",
    "    \n",
    "    precision_post_value = precision_post.compute()\n",
    "    recall_post_value = recall_post.compute()\n",
    "    f1_post_value = f1_post.compute()\n",
    "    iou_post_value = iou_post.compute()\n",
    "    \n",
    "    # Prepare metrics for reporting\n",
    "    metrics = {\n",
    "        'test_loss': avg_test_loss,\n",
    "        'pre_disaster': {\n",
    "            'class_names': pre_class_names,\n",
    "            'precision': precision_pre_value.cpu().numpy(),\n",
    "            'recall': recall_pre_value.cpu().numpy(),\n",
    "            'f1_score': f1_pre_value.cpu().numpy(),\n",
    "            'iou': iou_pre_value.cpu().numpy(),\n",
    "            'avg_precision': precision_pre_value.mean().item(),\n",
    "            'avg_recall': recall_pre_value.mean().item(),\n",
    "            'avg_f1': f1_pre_value.mean().item(),\n",
    "            'avg_iou': iou_pre_value.mean().item()\n",
    "        },\n",
    "        'post_disaster': {\n",
    "            'class_names': post_class_names,\n",
    "            'precision': precision_post_value.cpu().numpy(),\n",
    "            'recall': recall_post_value.cpu().numpy(),\n",
    "            'f1_score': f1_post_value.cpu().numpy(),\n",
    "            'iou': iou_post_value.cpu().numpy(),\n",
    "            'avg_precision': precision_post_value.mean().item(),\n",
    "            'avg_recall': recall_post_value.mean().item(),\n",
    "            'avg_f1': f1_post_value.mean().item(),\n",
    "            'avg_iou': iou_post_value.mean().item()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create confusion matrices\n",
    "    all_pre_preds = torch.cat(all_pre_preds).cpu().numpy()\n",
    "    all_pre_targets = torch.cat(all_pre_targets).cpu().numpy()\n",
    "    all_post_preds = torch.cat(all_post_preds).cpu().numpy()\n",
    "    all_post_targets = torch.cat(all_post_targets).cpu().numpy()\n",
    "    \n",
    "    return metrics, sample_images, (all_pre_preds, all_pre_targets, all_post_preds, all_post_targets)\n",
    "\n",
    "def save_metrics(metrics):\n",
    "    \"\"\"Save metrics to file\"\"\"\n",
    "    # Overall metrics\n",
    "    with open(RESULTS_DIR / \"metrics_summary.txt\", \"w\") as f:\n",
    "        f.write(f\"Test Loss: {metrics['test_loss']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Pre-Disaster Metrics:\\n\")\n",
    "        f.write(f\"Average Precision: {metrics['pre_disaster']['avg_precision']:.4f}\\n\")\n",
    "        f.write(f\"Average Recall: {metrics['pre_disaster']['avg_recall']:.4f}\\n\")\n",
    "        f.write(f\"Average F1-Score: {metrics['pre_disaster']['avg_f1']:.4f}\\n\")\n",
    "        f.write(f\"Average IoU: {metrics['pre_disaster']['avg_iou']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Post-Disaster Metrics:\\n\")\n",
    "        f.write(f\"Average Precision: {metrics['post_disaster']['avg_precision']:.4f}\\n\")\n",
    "        f.write(f\"Average Recall: {metrics['post_disaster']['avg_recall']:.4f}\\n\")\n",
    "        f.write(f\"Average F1-Score: {metrics['post_disaster']['avg_f1']:.4f}\\n\")\n",
    "        f.write(f\"Average IoU: {metrics['post_disaster']['avg_iou']:.4f}\\n\")\n",
    "    \n",
    "    # Class-specific metrics (pre-disaster)\n",
    "    pre_metrics_df = pd.DataFrame({\n",
    "        'Class': metrics['pre_disaster']['class_names'],\n",
    "        'Precision': metrics['pre_disaster']['precision'],\n",
    "        'Recall': metrics['pre_disaster']['recall'],\n",
    "        'F1-Score': metrics['pre_disaster']['f1_score'],\n",
    "        'IoU': metrics['pre_disaster']['iou']\n",
    "    })\n",
    "    pre_metrics_df.to_csv(RESULTS_DIR / \"pre_disaster_metrics.csv\", index=False)\n",
    "    \n",
    "    # Class-specific metrics (post-disaster)\n",
    "    post_metrics_df = pd.DataFrame({\n",
    "        'Class': metrics['post_disaster']['class_names'],\n",
    "        'Precision': metrics['post_disaster']['precision'],\n",
    "        'Recall': metrics['post_disaster']['recall'],\n",
    "        'F1-Score': metrics['post_disaster']['f1_score'],\n",
    "        'IoU': metrics['post_disaster']['iou']\n",
    "    })\n",
    "    post_metrics_df.to_csv(RESULTS_DIR / \"post_disaster_metrics.csv\", index=False)\n",
    "    \n",
    "    print(f\"Metrics saved to {RESULTS_DIR}\")\n",
    "\n",
    "def plot_confusion_matrices(prediction_data):\n",
    "    \"\"\"Plot and save confusion matrices\"\"\"\n",
    "    pre_preds, pre_targets, post_preds, post_targets = prediction_data\n",
    "    \n",
    "    # Compute confusion matrices\n",
    "    cm_pre = confusion_matrix(pre_targets.flatten(), pre_preds.flatten())\n",
    "    cm_post = confusion_matrix(post_targets.flatten(), post_preds.flatten())\n",
    "    \n",
    "    # Plot pre-disaster confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_pre, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=pre_class_names, yticklabels=pre_class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Pre-Disaster Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"pre_disaster_confusion_matrix.png\")\n",
    "    \n",
    "    # Plot post-disaster confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_post, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=post_class_names, yticklabels=post_class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Post-Disaster Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"post_disaster_confusion_matrix.png\")\n",
    "    \n",
    "    print(f\"Confusion matrices saved to {RESULTS_DIR}\")\n",
    "\n",
    "def visualize_samples(sample_images):\n",
    "    \"\"\"Visualize sample predictions\"\"\"\n",
    "    for i, sample in enumerate(sample_images):\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Display pre-disaster image\n",
    "        axs[0, 0].imshow(np.transpose(sample['pre_img'], (1, 2, 0)))\n",
    "        axs[0, 0].set_title('Pre-Disaster Image')\n",
    "        axs[0, 0].axis('off')\n",
    "        \n",
    "        # Display pre-disaster ground truth\n",
    "        axs[0, 1].imshow(sample['pre_mask'].squeeze(), cmap='tab10', vmin=0, vmax=1)\n",
    "        axs[0, 1].set_title('Pre-Disaster Ground Truth')\n",
    "        axs[0, 1].axis('off')\n",
    "        \n",
    "        # Display pre-disaster prediction\n",
    "        axs[0, 2].imshow(sample['pre_pred'], cmap='tab10', vmin=0, vmax=1)\n",
    "        axs[0, 2].set_title('Pre-Disaster Prediction')\n",
    "        axs[0, 2].axis('off')\n",
    "        \n",
    "        # Display post-disaster image\n",
    "        axs[1, 0].imshow(np.transpose(sample['post_img'], (1, 2, 0)))\n",
    "        axs[1, 0].set_title('Post-Disaster Image')\n",
    "        axs[1, 0].axis('off')\n",
    "        \n",
    "        # Display post-disaster ground truth\n",
    "        axs[1, 1].imshow(sample['post_mask'].squeeze(), cmap='tab10', vmin=0, vmax=5)\n",
    "        axs[1, 1].set_title('Post-Disaster Ground Truth')\n",
    "        axs[1, 1].axis('off')\n",
    "        \n",
    "        # Display post-disaster prediction\n",
    "        axs[1, 2].imshow(sample['post_pred'], cmap='tab10', vmin=0, vmax=5)\n",
    "        axs[1, 2].set_title('Post-Disaster Prediction')\n",
    "        axs[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(VISUALIZATION_DIR / f\"sample_{i+1}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Sample visualizations saved to {VISUALIZATION_DIR}\")\n",
    "\n",
    "def create_class_distribution_plots(prediction_data):\n",
    "    \"\"\"Create and save class distribution plots\"\"\"\n",
    "    pre_preds, pre_targets, post_preds, post_targets = prediction_data\n",
    "    \n",
    "    # Plot pre-disaster class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pre_target_counts = np.bincount(pre_targets.flatten(), minlength=2)\n",
    "    pre_pred_counts = np.bincount(pre_preds.flatten(), minlength=2)\n",
    "    \n",
    "    x = np.arange(len(pre_class_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, pre_target_counts, width, label='Ground Truth')\n",
    "    plt.bar(x + width/2, pre_pred_counts, width, label='Predicted')\n",
    "    \n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Pre-Disaster Class Distribution')\n",
    "    plt.xticks(x, pre_class_names)\n",
    "    plt.legend()\n",
    "    plt.savefig(RESULTS_DIR / \"pre_disaster_class_distribution.png\")\n",
    "    \n",
    "    # Plot post-disaster class distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    post_target_counts = np.bincount(post_targets.flatten(), minlength=6)\n",
    "    post_pred_counts = np.bincount(post_preds.flatten(), minlength=6)\n",
    "    \n",
    "    x = np.arange(len(post_class_names))\n",
    "    \n",
    "    plt.bar(x - width/2, post_target_counts, width, label='Ground Truth')\n",
    "    plt.bar(x + width/2, post_pred_counts, width, label='Predicted')\n",
    "    \n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Post-Disaster Class Distribution')\n",
    "    plt.xticks(x, post_class_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"post_disaster_class_distribution.png\")\n",
    "    \n",
    "    print(f\"Class distribution plots saved to {RESULTS_DIR}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting model evaluation...\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics, sample_images, prediction_data = evaluate_model()\n",
    "    \n",
    "    # Print summary metrics\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Test Loss: {metrics['test_loss']:.4f}\")\n",
    "    print(\"\\nPre-Disaster Metrics:\")\n",
    "    print(f\"Average Precision: {metrics['pre_disaster']['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['pre_disaster']['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1-Score: {metrics['pre_disaster']['avg_f1']:.4f}\")\n",
    "    print(f\"Average IoU: {metrics['pre_disaster']['avg_iou']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPost-Disaster Metrics:\")\n",
    "    print(f\"Average Precision: {metrics['post_disaster']['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['post_disaster']['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1-Score: {metrics['post_disaster']['avg_f1']:.4f}\")\n",
    "    print(f\"Average IoU: {metrics['post_disaster']['avg_iou']:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    save_metrics(metrics)\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_confusion_matrices(prediction_data)\n",
    "    visualize_samples(sample_images)\n",
    "    create_class_distribution_plots(prediction_data)\n",
    "    \n",
    "    print(\"\\nEvaluation completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

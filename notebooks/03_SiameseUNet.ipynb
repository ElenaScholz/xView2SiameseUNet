{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "from typing import Tuple, Dict, List\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "#from utils.helper import iterate_through_dir\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.v2 as v2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "#from utils.datasetClass import xView2Dataset, image_transform, transform, collate_fn\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from utils.model import UNet_ResNet50, SiameseUnet, train_step, val_step\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils.dataset import transform, image_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief introduction in \n",
    "TODO - Giver overview over Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define all Pathes and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\dss\\dsstbyfs02\\pn49ci\\pn49ci-dss-0022\\data\\xview2\\xView2_Experiments\n"
     ]
    }
   ],
   "source": [
    "# HPC Terrabyte\n",
    "# adapt the user to your needs\n",
    "USER = \"di97ren\"\n",
    "# keep the following unchanged\n",
    "ROOT = Path(\"/dss/dsstbyfs02/pn49ci/pn49ci-dss-0022\")\n",
    "USER_PATH = ROOT / f\"users/{USER}\"\n",
    "DATA_PATH = ROOT / \"data\"\n",
    "\n",
    "\n",
    "# Configure the path to the xview2 dataset for your environment\n",
    "DATASET_ROOT = DATA_PATH / \"xview2\"\n",
    "\n",
    "TRAIN_ROOT = DATASET_ROOT / \"tier1\"\n",
    "TRAIN_IMG = TRAIN_ROOT / \"png_images\"\n",
    "TRAIN_TARGET = TRAIN_ROOT / 'targets'\n",
    "\n",
    "VAL_ROOT = DATASET_ROOT / \"hold\"\n",
    "VAL_IMG = VAL_ROOT / \"png_images\"\n",
    "VAL_TARGET = VAL_ROOT / 'targets'\n",
    "\n",
    "\n",
    "TEST_ROOT = DATASET_ROOT / \"test\"\n",
    "TEST_IMG = TEST_ROOT / \"png_images\"\n",
    "\n",
    "\n",
    "\n",
    "# Pathes to store the experiment information in:\n",
    "EXPERIMENT_GROUP = \"xView2_Experiments\"\n",
    "EXPERIMENT_ID = \"001\"\n",
    "\n",
    "EXPERIMENT_DIR = Path(f\"/dss/dsstbyfs02/pn49ci/pn49ci-dss-0022/data/xview2/{EXPERIMENT_GROUP}\")\n",
    "EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINTS_DIR = EXPERIMENT_DIR / f\"checkpoints/{EXPERIMENT_GROUP}\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_functions import iterate_through_dir\n",
    "# Take a Look what is inside our ROOT Directory and how many files are inside the other folders:\n",
    "iterate_through_dir(ROOT)\n",
    "iterate_through_dir(TRAIN_ROOT)\n",
    "iterate_through_dir(VAL_ROOT)\n",
    "iterate_through_dir(TEST_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class and Collate Function\n",
    "\n",
    "TODO- Explain the dataset class and collate function\n",
    "TODO - Also Explain the image transform and transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xView2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 png_path: str,\n",
    "                 target_path: callable = None,\n",
    "                 transform: callable = None,\n",
    "                 image_transform: callable = None,\n",
    "                 inference = False):\n",
    "\n",
    "        \n",
    "        self.png_path = png_path\n",
    "        self.target_path = target_path\n",
    "        self.transform = transform\n",
    "        self.image_transform = image_transform\n",
    "        self.inference = inference\n",
    "\n",
    "        \n",
    "\n",
    "        # get all pre-disaster images:\n",
    "        self.pre_images = sorted(self.png_path.glob(\"*_pre_disaster.png\"))\n",
    "        \n",
    "        self.pairs = [] #\n",
    "\n",
    "        for pre_img_path in self.pre_images:\n",
    "            post_img_path = self.png_path / pre_img_path.name.replace(\"_pre_disaster\", \"_post_disaster\")\n",
    "\n",
    "            if self.inference: \n",
    "                if post_img_path.exists():\n",
    "                    self.pairs.append((pre_img_path, post_img_path))\n",
    "            else: \n",
    "                # Nur im Trainingsmodus benötigen wir target_path\n",
    "                if self.target_path is None:\n",
    "                    raise ValueError(\"target_path must be provided when not in inference mode\")\n",
    "                    \n",
    "                post_target_path = self.target_path / pre_img_path.name.replace(\"_pre_disaster\", \"_post_disaster\")\n",
    "                pre_target_path = self.target_path / pre_img_path.name\n",
    "\n",
    "                if post_img_path.exists() and post_target_path.exists() and pre_target_path.exists():\n",
    "                    self.pairs.append((pre_img_path, post_img_path, pre_target_path, post_target_path))\n",
    "\n",
    "        assert len(self.pairs) > 0, \"No matching image-pairs found!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if self.inference:\n",
    "            pre_img_path, post_img_path = self.pairs[index]\n",
    "\n",
    "            # Load images\n",
    "            pre_img = Image.open(pre_img_path).convert(\"RGB\")\n",
    "            post_img = Image.open(post_img_path).convert(\"RGB\")\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            pre_img = np.array(pre_img, dtype=np.float32) / 255.0\n",
    "            post_img = np.array(post_img, dtype=np.float32) / 255.0\n",
    "\n",
    "            # Convert to Tensor\n",
    "            pre_img = torch.tensor(pre_img).permute(2, 0, 1)  # (H, W, C) → (C, H, W)\n",
    "            post_img = torch.tensor(post_img).permute(2, 0, 1)\n",
    "            \n",
    "            if self.image_transform:\n",
    "                pre_img = self.image_transform(pre_img)\n",
    "                post_img = self.image_transform(post_img)\n",
    "\n",
    "            return pre_img, post_img, pre_img_path.name, post_img_path.name\n",
    "\n",
    "        else:\n",
    "            pre_img_path, post_img_path, pre_target_path, post_target_path = self.pairs[index]\n",
    "\n",
    "            # load images and target masks with \n",
    "            \n",
    "            pre_img = Image.open(pre_img_path).convert(\"RGB\")\n",
    "            post_img = Image.open(post_img_path).convert(\"RGB\")\n",
    "            pre_target_mask = Image.open(pre_target_path).convert('L')\n",
    "            post_target_mask = Image.open(post_target_path).convert('L')\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            pre_img = np.array(pre_img, dtype=np.float32) / 255.0\n",
    "            post_img = np.array(post_img, dtype=np.float32) / 255.0\n",
    "            pre_target_mask = np.array(pre_target_mask, dtype=np.float32)\n",
    "            post_target_mask = np.array(post_target_mask, dtype=np.float32)\n",
    "\n",
    "            # convert to Tensor\n",
    "            pre_img = torch.tensor(pre_img).permute(2, 0, 1)  # (H, W, C) → (C, H, W)\n",
    "            post_img = torch.tensor(post_img).permute(2, 0, 1)\n",
    "            pre_target_mask = torch.tensor(pre_target_mask).unsqueeze(0)  # (H, W) → (1, H, W)\n",
    "            post_target_mask = torch.tensor(post_target_mask).unsqueeze(0)\n",
    "\n",
    "            # Transformation (optional)\n",
    "            if self.transform:\n",
    "                stack = torch.cat([pre_img, post_img, pre_target_mask, post_target_mask], dim=0)  # (8, H, W)\n",
    "                stack = self.transform(stack)\n",
    "\n",
    "                pre_img, post_img, pre_target_mask, post_target_mask = stack[:3], stack[3:6], stack[6:7], stack[7:8]\n",
    "            \n",
    "            if self.image_transform:\n",
    "                # Nur auf Bilder Normalisierung anwenden\n",
    "                pre_img = self.image_transform(pre_img)\n",
    "                post_img = self.image_transform(post_img)\n",
    "\n",
    "            return pre_img, post_img, pre_target_mask, post_target_mask \n",
    "    \n",
    "def collate_fn_test(batch):\n",
    "    pre_imgs, post_imgs, pre_names, post_names = zip(*batch)\n",
    "    # Stapeln der Tensoren entlang der Batch-Dimension (erste Dimension)\n",
    "    pre_imgs = torch.stack(pre_imgs, dim=0)\n",
    "    post_imgs = torch.stack(post_imgs, dim=0)\n",
    "\n",
    "    return pre_imgs, post_imgs, pre_names, post_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the Dataset and it structures:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = xView2Dataset(png_path = TRAIN_IMG,\n",
    "                 target_path = TRAIN_TARGET,\n",
    "                 transform = transform(),\n",
    "                 image_transform = image_transform()\n",
    "                 )\n",
    "\n",
    "val_dataset = xView2Dataset(png_path = VAL_IMG ,\n",
    "                 target_path = VAL_TARGET,\n",
    "                 transform = transform(),\n",
    "                 image_transform = image_transform()\n",
    "                 )\n",
    "\n",
    "train_dataset, val_dataset\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Inspect the dataset sample - show formats, length, masks\n",
    "# TODO - Plot one image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.viz import create_Control_plots\n",
    "\n",
    "create_Control_plots(train_dataset[0]) # Inspect the first sample - Keep in mind, that the images are already transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model design\n",
    "TODO: Overview (maybe a diagram)\n",
    "TODO: Why did i choose the Siamese UNet and Why the ResNet50 (compare before and after images, and strong feature extraction of ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_ResNet50(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load ResNet50 als Encoder\n",
    "        resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "        \n",
    "        # Encoder-Pfad\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "        \n",
    "        # Decoder mit progressivem Upsampling\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 1024, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Finale Klassifikationsschicht\n",
    "        self.final_conv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Speichere Ursprungsgröße\n",
    "        original_size = x.shape[2:]\n",
    "        \n",
    "        try:\n",
    "            # Encoder-Durchgang\n",
    "            features = self.encoder(x)\n",
    "            #print(f\"Encoder Output Shape: {features.shape}\")\n",
    "\n",
    "            # Decoder-Durchgang\n",
    "            decoder_out = self.decoder(features)\n",
    "            #print(f\"Decoder Output Shape: {decoder_out.shape}\")\n",
    "\n",
    "            # Finale Konvolution\n",
    "            out = self.final_conv(decoder_out)\n",
    "            #print(f\"Final Conv Output Shape: {out.shape}\")\n",
    "\n",
    "            # Upsample auf Originalgröße\n",
    "            out = F.interpolate(out, size=original_size, mode='bilinear', align_corners=False)\n",
    "            #print(f\"Final Interpolated Output Shape: {out.shape}\")\n",
    "\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseUnet(nn.Module):\n",
    "    def __init__(self, num_pre_classes=2, num_post_classes=6):\n",
    "        super(SiameseUnet, self).__init__()\n",
    "\n",
    "        self.unet_preDisaster = UNet_ResNet50(n_class=num_pre_classes)\n",
    "        self.unet_postDisaster = UNet_ResNet50(n_class=num_post_classes)\n",
    "\n",
    "        # Fusion-Layer kombiniert prä- und post-Klassifikationen\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Conv2d(num_pre_classes + num_post_classes, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, num_pre_classes + num_post_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pre_image, post_image):\n",
    "        pre_output = self.unet_preDisaster(pre_image)\n",
    "        post_output = self.unet_postDisaster(post_image)\n",
    "\n",
    "        # Konkatenieren der Ausgaben\n",
    "        fused_output = torch.cat([pre_output, post_output], dim=1)\n",
    "\n",
    "        # Fusion der Features\n",
    "        final_output = self.fusion_layer(fused_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with class imbalance\n",
    "TODO Describe the problem: class imbalance in segmentation.\n",
    "\n",
    "TODO How your custom loss handles it.\n",
    "\n",
    "TODO Maybe show formulas or pseudo-code.\n",
    "\n",
    "TODO If you created extra helper functions (like weighted cross-entropy, focal loss, etc.), you can explain them here too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        # Stelle sicher, dass alpha den richtigen Typ hat\n",
    "        if alpha is not None:\n",
    "            self.alpha = alpha.float() if isinstance(alpha, torch.Tensor) else torch.tensor(alpha, dtype=torch.float32)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Stelle sicher, dass die Eingaben den richtigen Typ haben\n",
    "        inputs = inputs.float()  # Wandle in float32 um\n",
    "        targets = targets.long()  # Wandle in long um\n",
    "        \n",
    "        B, C, H, W = inputs.size()\n",
    "        \n",
    "        # Reshape inputs and targets\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Stelle sicher, dass alpha zu float konvertiert wird und auf demselben Gerät liegt\n",
    "        weight = None\n",
    "        if self.alpha is not None:\n",
    "            weight = self.alpha.to(inputs.device).float()\n",
    "        \n",
    "        # Berechne Cross-Entropy-Loss\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=weight)\n",
    "        \n",
    "        # Berechne Wahrscheinlichkeiten\n",
    "        probs = F.softmax(inputs, dim=1)\n",
    "        probs_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Wende Focal-Gewichtung an\n",
    "        focal_weight = (1 - probs_t) ** self.gamma\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "        \n",
    "        # Wende reduction an\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:  # 'none'\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combined loss function for training\n",
    "def combined_loss_function(outputs, pre_masks, post_masks):\n",
    "    pre_outputs = outputs[:, :2]  # First 2 channels\n",
    "    post_outputs = outputs[:, 2:]  # Remaining channels\n",
    "    \n",
    "    # Calculate focal losses\n",
    "    pre_loss = focal_loss_pre(pre_outputs, pre_masks)\n",
    "    post_loss = focal_loss_post(post_outputs, post_masks)\n",
    "    \n",
    "    # Combine losses (you can adjust weights)\n",
    "    total_loss = pre_loss + post_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weights and class counts\n",
    "TODO \n",
    "ACHTUNG EINMAL DIESEN TEIL HIER DURCHLAUFEN LASSEN UND SPEICHERN - SO KANN ICH DANN DAS MODELL EINMALIG LADEN:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training_preparations import (\n",
    "    calculate_class_counts, save_class_counts, load_class_counts,\n",
    "    get_sample_weights, save_sample_weights, load_sample_weights\n",
    ")\n",
    "\n",
    "# Pathes to the files - Dont change\n",
    "class_counts_path = \"saved/class_counts.json\"\n",
    "sample_weights_path = \"saved/sample_weights.pt\"\n",
    "\n",
    "# If the distribution of Class Counts and the weights of the samples already exists load the results - other wise compute them once!\n",
    "if os.path.exists(class_counts_path):\n",
    "    print(\"Lade gespeicherte Class Counts...\")\n",
    "    pre_counts, post_counts = load_class_counts(class_counts_path)\n",
    "else:\n",
    "    print(\"Berechne Class Counts...\")\n",
    "    pre_counts, post_counts = calculate_class_counts(train_dataset)\n",
    "    save_class_counts(pre_counts, post_counts, class_counts_path)\n",
    "\n",
    "# Sample weights laden oder berechnen\n",
    "if os.path.exists(sample_weights_path):\n",
    "    print(\"Lade gespeicherte Sample Weights...\")\n",
    "    sample_weights = load_sample_weights(sample_weights_path)\n",
    "else:\n",
    "    print(\"Berechne Sample Weights...\")\n",
    "    sample_weights = get_sample_weights(train_dataset)\n",
    "    save_sample_weights(sample_weights, sample_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import MulticlassJaccardIndex\n",
    "\n",
    "# Updated train_step function\n",
    "def train_step(model, dataloader, optimizer, epoch):\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    precision_pre = MulticlassPrecision(num_classes=2).to(device)\n",
    "    recall_pre = MulticlassRecall(num_classes=2).to(device)\n",
    "    f1_pre = MulticlassF1Score(num_classes=2).to(device)\n",
    "    iou_pre = MulticlassJaccardIndex(num_classes=2, average='macro').to(device)\n",
    "\n",
    "    precision_post = MulticlassPrecision(num_classes=6).to(device)\n",
    "    recall_post = MulticlassRecall(num_classes=6).to(device)\n",
    "    f1_post = MulticlassF1Score(num_classes=6).to(device)\n",
    "    iou_post = MulticlassJaccardIndex(num_classes=6, average='macro').to(device)\n",
    "\n",
    "\n",
    "    for pre_imgs, post_imgs, pre_masks, post_masks in dataloader:\n",
    "        X_pre = pre_imgs.to(device)\n",
    "        y_pre = pre_masks.to(device)\n",
    "        X_post = post_imgs.to(device)\n",
    "        y_post = post_masks.to(device)\n",
    "        X_pre = X_pre.float()\n",
    "        X_post = X_post.float()\n",
    "        # Prepare masks for metrics\n",
    "        y_pre_metric = y_pre.squeeze(1).long()\n",
    "        y_post_metric = y_post.squeeze(1).long()\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(X_pre, X_post)\n",
    "        pred.float()\n",
    "        # Calculate loss using combined loss function\n",
    "        loss = combined_loss_function(pred, y_pre_metric, y_post_metric)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "                # Im train_step nach loss.backward()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"gradients/{name}\", param.grad, epoch)\n",
    "                writer.add_histogram(f\"weights/{name}\", param, epoch)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        # Track loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        pre_pred = torch.argmax(pred[:, :2], dim=1)\n",
    "        post_pred = torch.argmax(pred[:, 2:], dim=1)\n",
    "        \n",
    "        # Update metrics\n",
    "        precision_pre.update(pred[:, :2], y_pre_metric)\n",
    "        recall_pre.update(pred[:, :2], y_pre_metric)\n",
    "        f1_pre.update(pred[:, :2], y_pre_metric)\n",
    "        iou_pre.update(pred[:, :2], y_pre_metric)\n",
    "\n",
    "        precision_post.update(pred[:, 2:], y_post_metric)\n",
    "        recall_post.update(pred[:, 2:], y_post_metric)\n",
    "        f1_post.update(pred[:, 2:], y_post_metric)\n",
    "        iou_post.update(pred[:, 2:], y_post_metric)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    \n",
    "    precision_pre_value = precision_pre.compute()\n",
    "    recall_pre_value = recall_pre.compute()\n",
    "    f1_pre_value = f1_pre.compute()\n",
    "    mean_iou_pre = iou_pre.compute()\n",
    "\n",
    "    precision_post_value = precision_post.compute()\n",
    "    recall_post_value = recall_post.compute()\n",
    "    f1_post_value = f1_post.compute()\n",
    "    mean_iou_post = iou_post.compute()\n",
    "\n",
    "    # # Calculate average IoU\n",
    "    # avg_iou_pre = {cls: val / sample_count for cls, val in iou_pre.items()}\n",
    "    # avg_iou_post = {cls: val / sample_count for cls, val in iou_post.items()}\n",
    "    # mean_iou_pre = sum(avg_iou_pre.values()) / len(avg_iou_pre)\n",
    "    # mean_iou_post = sum(avg_iou_post.values()) / len(avg_iou_post)\n",
    "    \n",
    "    # TensorBoard Logging\n",
    "    writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "    \n",
    "    # Logging for pre-disaster metrics\n",
    "    writer.add_scalar(\"Precision_Pre/Train\", precision_pre_value.mean(), epoch)\n",
    "    writer.add_scalar(\"Recall_Pre/Train\", recall_pre_value.mean(), epoch)\n",
    "    writer.add_scalar(\"F1_Score_Pre/Train\", f1_pre_value.mean(), epoch)\n",
    "    writer.add_scalar(\"IoU_Pre/Train\", mean_iou_pre, epoch)\n",
    "    \n",
    "    \n",
    "    # Logging for post-disaster metrics\n",
    "    writer.add_scalar(\"Precision_Post/Train\", precision_post_value.mean(), epoch)\n",
    "    writer.add_scalar(\"Recall_Post/Train\", recall_post_value.mean(), epoch)\n",
    "    writer.add_scalar(\"F1_Score_Post/Train\", f1_post_value.mean(), epoch)\n",
    "    writer.add_scalar(\"IoU_Post/Train\", mean_iou_post, epoch)\n",
    "    \n",
    "\n",
    "    return avg_train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated val_step function\n",
    "def val_step(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    precision_pre = MulticlassPrecision(num_classes=2).to(device)\n",
    "    recall_pre = MulticlassRecall(num_classes=2).to(device)\n",
    "    f1_pre = MulticlassF1Score(num_classes=2).to(device)\n",
    "    iou_pre = MulticlassJaccardIndex(num_classes=2, average='macro').to(device)\n",
    "    precision_post = MulticlassPrecision(num_classes=6).to(device)\n",
    "    recall_post = MulticlassRecall(num_classes=6).to(device)\n",
    "    f1_post = MulticlassF1Score(num_classes=6).to(device)\n",
    "    iou_post = MulticlassJaccardIndex(num_classes=6, average='macro').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pre_imgs, post_imgs, pre_masks, post_masks in dataloader:\n",
    "            X_pre = pre_imgs.to(device)\n",
    "            y_pre = pre_masks.to(device)\n",
    "            X_post = post_imgs.to(device)\n",
    "            y_post = post_masks.to(device)\n",
    "\n",
    "                        # Convert tensors to float32\n",
    "            X_pre = X_pre.float()\n",
    "            X_post = X_post.float()\n",
    "\n",
    "            \n",
    "            # Prepare masks for metrics\n",
    "            y_pre_metric = y_pre.squeeze(1).long()\n",
    "            y_post_metric = y_post.squeeze(1).long()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(X_pre, X_post)\n",
    "            pred= pred.float()\n",
    "            # Calculate loss using combined loss function\n",
    "            loss = combined_loss_function(pred, y_pre_metric, y_post_metric)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            pre_pred = torch.argmax(pred[:, :2], dim=1)\n",
    "            post_pred = torch.argmax(pred[:, 2:], dim=1)\n",
    "            \n",
    "            # Update metrics\n",
    "            precision_pre.update(pred[:, :2], y_pre_metric)\n",
    "            recall_pre.update(pred[:, :2], y_pre_metric)\n",
    "            f1_pre.update(pred[:, :2], y_pre_metric)\n",
    "            iou_pre.update(pred[:, :2], y_pre_metric)\n",
    "\n",
    "            precision_post.update(pred[:, 2:], y_post_metric)\n",
    "            recall_post.update(pred[:, 2:], y_post_metric)\n",
    "            f1_post.update(pred[:, 2:], y_post_metric)\n",
    "            iou_post.update(pred[:, :2], y_post_metric)\n",
    "\n",
    "            # Calculate IoU for each class\n",
    "            batch_size = y_pre.size(0)\n",
    "            sample_count += batch_size\n",
    "\n",
    "         \n",
    "            # for b in range(batch_size):\n",
    "            #     # Pre-disaster IoU\n",
    "            #     for cls in [0, 1]:\n",
    "            #         iou_pre[cls] += calculate_iou(pre_pred[b].cpu(), y_pre_metric[b].cpu(), cls)\n",
    "                \n",
    "            #     # Post-disaster IoU\n",
    "            #     for cls in range(6):\n",
    "            #         iou_post[cls] += calculate_iou(post_pred[b].cpu(), y_post_metric[b].cpu(), cls)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    \n",
    "    precision_pre_value = precision_pre.compute().cpu().numpy()\n",
    "    recall_pre_value = recall_pre.compute().cpu().numpy()\n",
    "    f1_pre_value = f1_pre.compute().cpu().numpy()\n",
    "    mean_iou_pre = iou_pre.compute()\n",
    "\n",
    "    precision_post_value = precision_post.compute().cpu().numpy()\n",
    "    recall_post_value = recall_post.compute().cpu().numpy()\n",
    "    f1_post_value = f1_post.compute().cpu().numpy()\n",
    "    mean_iou_post = iou_post.compute()\n",
    "   \n",
    "    \n",
    "    # TensorBoard Logging\n",
    "    writer.add_scalar(\"Loss/Val\", avg_val_loss, epoch)\n",
    "    \n",
    "    # Logging for pre-disaster metrics\n",
    "    writer.add_scalar(\"Precision_Pre/Val\", precision_pre_value.mean(), epoch)\n",
    "    writer.add_scalar(\"Recall_Pre/Val\", recall_pre_value.mean(), epoch)\n",
    "    writer.add_scalar(\"F1_Score_Pre/Val\", f1_pre_value.mean(), epoch)\n",
    "    writer.add_scalar(\"IoU_Pre/Val\", mean_iou_pre, epoch)\n",
    "    \n",
    "    \n",
    "    # Logging for post-disaster metrics\n",
    "    writer.add_scalar(\"Precision_Post/Val\", precision_post_value.mean(), epoch)\n",
    "    writer.add_scalar(\"Recall_Post/Val\", recall_post_value.mean(), epoch)\n",
    "    writer.add_scalar(\"F1_Score_Post/Val\", f1_post_value.mean(), epoch)\n",
    "    writer.add_scalar(\"IoU_Post/Val\", mean_iou_post, epoch)\n",
    "    \n",
    "    \n",
    "    return avg_val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "This notebook is intended to depict the results of inference for the xView2 Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.inference_step import inference\n",
    "from utils.helperfunctions import load_checkpoint, find_best_checkpoint, get_data_folder\n",
    "from utils.dataset import xView2Dataset, collate_fn_test, image_transform\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import yaml\n",
    "from model.siameseNetwork import SiameseUnet\n",
    "from model.uNet import UNet_ResNet50\n",
    "from model.loss import FocalLoss, combined_loss_function\n",
    "import torch\n",
    "base_dir = Path(os.getcwd()).parent  # Gehe einen Ordner zurück vom aktuellen Arbeitsverzeichnis\n",
    "config_path = base_dir / \"notebooks\" / \"config1.yaml\"\n",
    "print(base_dir)\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all pathes and create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_ROOT, TEST_ROOT, VAL_IMG, TEST_LABEL, TEST_TARGET, TEST_PNG_IMAGES = get_data_folder(\"test\", main_dataset = False)\n",
    "\n",
    "test_dataset = xView2Dataset(png_path=TEST_PNG_IMAGES,\n",
    "image_transform = image_transform(), inference = True)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size = 32,\n",
    "    collate_fn = collate_fn_test,\n",
    "    shuffle = False,\n",
    "    num_workers = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = config[\"data\"][\"user\"]\n",
    "#USER_PATH = Path(f\"/dss/dsstbyfs02/pn49ci/pn49ci-dss-0022/users/{USER}\")\n",
    "USER_HOME_PATH = Path(f\"/dss/dsshome1/08/{USER}\")\n",
    "\n",
    "# Pathes to store experiment informations in:\n",
    "EXPERIMENT_GROUP = config[\"data\"][\"experiment_group\"]\n",
    "EXPERIMENT_ID = config[\"data\"][\"experiment_id\"]\n",
    "EXPERIMENT_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"tensorboard_logs\" / EXPERIMENT_ID\n",
    "EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(EXPERIMENT_DIR)\n",
    "\n",
    "# Auch Checkpoints-Verzeichnis erstellen\n",
    "CHECKPOINTS_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"checkpoints\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Logfiles-Verzeichnis erstellen\n",
    "LOGFILES_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"logfiles\" / EXPERIMENT_ID\n",
    "LOGFILES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Logfiles werden gespeichert in: {LOGFILES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Modell initialisieren\n",
    "model = SiameseUnet(num_pre_classes=2, num_post_classes=6)\n",
    "model.to(device)\n",
    "\n",
    "best_checkpoint_path = find_best_checkpoint(CHECKPOINTS_DIR, EXPERIMENT_ID)\n",
    "# Besten Checkpoint laden\n",
    "model = load_checkpoint(model, best_checkpoint_path)\n",
    "results = inference(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.viz import vizualize_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualize_predictions(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/08/di97ren/04-geo-oma24/xView2SiameseUNet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import subprocess\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "import yaml\n",
    "\n",
    "base_dir = Path(os.getcwd()).parent  # Gehe einen Ordner zurück vom aktuellen Arbeitsverzeichnis\n",
    "config_path = base_dir / \"notebooks\" / \"config1.yaml\"\n",
    "print(base_dir)\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference-Ergebnisse werden gespeichert in: /dss/dsshome1/08/di97ren/xView2_all_data/inference_results/001\n"
     ]
    }
   ],
   "source": [
    "USER = config[\"data\"][\"user\"]\n",
    "USER_HOME_PATH = Path(f\"/dss/dsshome1/08/{USER}\")\n",
    "\n",
    "# Pathes to store experiment informations in:\n",
    "EXPERIMENT_GROUP = config[\"data\"][\"experiment_group\"]\n",
    "EXPERIMENT_ID = config[\"data\"][\"experiment_id\"]\n",
    "\n",
    "USER_HOME_PATH = Path(f\"/dss/dsshome1/08/{USER}\")\n",
    "RESULTS_DIR = USER_HOME_PATH / EXPERIMENT_GROUP / \"inference_results\" / EXPERIMENT_ID\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Inference-Ergebnisse werden gespeichert in: {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_results = list(RESULTS_DIR.glob(\"inference_results_*.pkl\"))\n",
    "if existing_results:\n",
    "    print(\"Gefundene bestehende Inference-Ergebnisse:\")\n",
    "    for i, result_file in enumerate(existing_results):\n",
    "        file_time = time.ctime(result_file.stat().st_mtime)\n",
    "        file_size = result_file.stat().st_size / (1024 * 1024)  # MB\n",
    "        print(f\"{i+1}. {result_file.name} - {file_time} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    use_existing = input(\"\\nBestehende Ergebnisse verwenden? (y/n): \").lower()\n",
    "    if use_existing == 'y':\n",
    "        idx = int(input(\"Nummer des zu verwendenden Ergebnisses eingeben: \")) - 1\n",
    "        results_file = existing_results[idx]\n",
    "        with open(results_file, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        print(f\"Ergebnisse aus {results_file} geladen.\")\n",
    "    else:\n",
    "        # Inference-Skript ausführen\n",
    "        run_inference = True\n",
    "else:\n",
    "    run_inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Inference-Prozess...\n",
      "Fehler beim Ausführen des Inference-Skripts:\n",
      "\n",
      "Running inference:   0%|          | 0/234 [00:00<?, ?batch/s]\n",
      "Running inference:   0%|          | 0/234 [00:03<?, ?batch/s, Samples=8, Avg time/batch=2.1043s]\n",
      "Running inference:   0%|          | 1/234 [00:03<13:05,  3.37s/batch, Samples=8, Avg time/batch=2.1043s]\n",
      "Running inference:   0%|          | 1/234 [00:03<13:05,  3.37s/batch, Samples=16, Avg time/batch=1.2142s]\n",
      "Running inference:   1%|          | 2/234 [00:03<06:16,  1.62s/batch, Samples=16, Avg time/batch=1.2142s]\n",
      "Running inference:   1%|          | 2/234 [00:04<06:16,  1.62s/batch, Samples=24, Avg time/batch=0.9215s]\n",
      "Running inference:   1%|▏         | 3/234 [00:04<04:05,  1.06s/batch, Samples=24, Avg time/batch=0.9215s]\n",
      "Running inference:   1%|▏         | 3/234 [00:04<04:05,  1.06s/batch, Samples=32, Avg time/batch=0.7856s]\n",
      "Running inference:   2%|▏         | 4/234 [00:04<03:07,  1.23batch/s, Samples=32, Avg time/batch=0.7856s]\n",
      "Running inference:   2%|▏         | 4/234 [00:05<03:07,  1.23batch/s, Samples=40, Avg time/batch=0.7006s]\n",
      "Running inference:   2%|▏         | 5/234 [00:05<02:33,  1.49batch/s, Samples=40, Avg time/batch=0.7006s]\n",
      "Running inference:   2%|▏         | 5/234 [00:05<02:33,  1.49batch/s, Samples=48, Avg time/batch=0.6502s]\n",
      "Running inference:   3%|▎         | 6/234 [00:05<02:16,  1.67batch/s, Samples=48, Avg time/batch=0.6502s]\n",
      "Running inference:   3%|▎         | 6/234 [00:05<02:16,  1.67batch/s, Samples=56, Avg time/batch=0.6069s]\n",
      "Running inference:   3%|▎         | 7/234 [00:05<02:01,  1.87batch/s, Samples=56, Avg time/batch=0.6069s]\n",
      "Running inference:   3%|▎         | 7/234 [00:06<02:01,  1.87batch/s, Samples=64, Avg time/batch=0.5671s]\n",
      "Running inference:   3%|▎         | 8/234 [00:06<01:47,  2.11batch/s, Samples=64, Avg time/batch=0.5671s]\n",
      "Running inference:   3%|▎         | 8/234 [00:06<01:47,  2.11batch/s, Samples=72, Avg time/batch=0.5455s]\n",
      "Running inference:   4%|▍         | 9/234 [00:06<01:43,  2.18batch/s, Samples=72, Avg time/batch=0.5455s]\n",
      "Running inference:   4%|▍         | 9/234 [00:07<01:43,  2.18batch/s, Samples=80, Avg time/batch=0.5241s]\n",
      "Running inference:   4%|▍         | 10/234 [00:07<01:37,  2.29batch/s, Samples=80, Avg time/batch=0.5241s]\n",
      "Running inference:   4%|▍         | 10/234 [00:07<01:37,  2.29batch/s, Samples=88, Avg time/batch=0.5072s]\n",
      "Running inference:   5%|▍         | 11/234 [00:07<01:34,  2.36batch/s, Samples=88, Avg time/batch=0.5072s]\n",
      "Running inference:   5%|▍         | 11/234 [00:07<01:34,  2.36batch/s, Samples=96, Avg time/batch=0.4964s]\n",
      "Running inference:   5%|▌         | 12/234 [00:07<01:34,  2.34batch/s, Samples=96, Avg time/batch=0.4964s]\n",
      "Running inference:   5%|▌         | 12/234 [00:08<01:34,  2.34batch/s, Samples=104, Avg time/batch=0.4828s]\n",
      "Running inference:   6%|▌         | 13/234 [00:08<01:31,  2.42batch/s, Samples=104, Avg time/batch=0.4828s]\n",
      "Running inference:   6%|▌         | 13/234 [00:08<01:31,  2.42batch/s, Samples=112, Avg time/batch=0.4741s]\n",
      "Running inference:   6%|▌         | 14/234 [00:08<01:31,  2.41batch/s, Samples=112, Avg time/batch=0.4741s]\n",
      "Running inference:   6%|▌         | 14/234 [00:09<01:31,  2.41batch/s, Samples=120, Avg time/batch=0.4685s]\n",
      "Running inference:   6%|▋         | 15/234 [00:09<01:32,  2.35batch/s, Samples=120, Avg time/batch=0.4685s]\n",
      "Running inference:   6%|▋         | 15/234 [00:09<01:32,  2.35batch/s, Samples=128, Avg time/batch=0.4622s]\n",
      "Running inference:   7%|▋         | 16/234 [00:09<01:32,  2.35batch/s, Samples=128, Avg time/batch=0.4622s]\n",
      "Running inference:   7%|▋         | 16/234 [00:09<01:32,  2.35batch/s, Samples=136, Avg time/batch=0.4559s]\n",
      "Running inference:   7%|▋         | 17/234 [00:09<01:31,  2.38batch/s, Samples=136, Avg time/batch=0.4559s]\n",
      "Running inference:   7%|▋         | 17/234 [00:10<01:31,  2.38batch/s, Samples=144, Avg time/batch=0.4498s]\n",
      "Running inference:   8%|▊         | 18/234 [00:10<01:29,  2.41batch/s, Samples=144, Avg time/batch=0.4498s]\n",
      "Running inference:   8%|▊         | 18/234 [00:10<01:29,  2.41batch/s, Samples=152, Avg time/batch=0.4456s]\n",
      "Running inference:   8%|▊         | 19/234 [00:10<01:29,  2.39batch/s, Samples=152, Avg time/batch=0.4456s]\n",
      "Running inference:   8%|▊         | 19/234 [00:11<01:29,  2.39batch/s, Samples=160, Avg time/batch=0.4538s]\n",
      "Running inference:   9%|▊         | 20/234 [00:11<01:45,  2.02batch/s, Samples=160, Avg time/batch=0.4538s]\n",
      "Running inference:   9%|▊         | 20/234 [00:11<01:45,  2.02batch/s, Samples=168, Avg time/batch=0.4499s]\n",
      "Running inference:   9%|▉         | 21/234 [00:11<01:41,  2.10batch/s, Samples=168, Avg time/batch=0.4499s]\n",
      "Running inference:   9%|▉         | 21/234 [00:12<01:41,  2.10batch/s, Samples=176, Avg time/batch=0.4430s]\n",
      "Running inference:   9%|▉         | 22/234 [00:12<01:33,  2.27batch/s, Samples=176, Avg time/batch=0.4430s]\n",
      "Running inference:   9%|▉         | 22/234 [00:12<01:33,  2.27batch/s, Samples=184, Avg time/batch=0.4359s]\n",
      "Running inference:  10%|▉         | 23/234 [00:12<01:24,  2.49batch/s, Samples=184, Avg time/batch=0.4359s]\n",
      "Running inference:  10%|▉         | 23/234 [00:12<01:24,  2.49batch/s, Samples=192, Avg time/batch=0.4325s]\n",
      "Running inference:  10%|█         | 24/234 [00:12<01:26,  2.43batch/s, Samples=192, Avg time/batch=0.4325s]\n",
      "Running inference:  10%|█         | 24/234 [00:13<01:26,  2.43batch/s, Samples=200, Avg time/batch=0.4294s]\n",
      "Running inference:  11%|█         | 25/234 [00:13<01:25,  2.43batch/s, Samples=200, Avg time/batch=0.4294s]\n",
      "Running inference:  11%|█         | 25/234 [00:13<01:25,  2.43batch/s, Samples=208, Avg time/batch=0.4257s]\n",
      "Running inference:  11%|█         | 26/234 [00:13<01:24,  2.48batch/s, Samples=208, Avg time/batch=0.4257s]\n",
      "Running inference:  11%|█         | 26/234 [00:14<01:24,  2.48batch/s, Samples=216, Avg time/batch=0.4235s]\n",
      "Running inference:  12%|█▏        | 27/234 [00:14<01:24,  2.44batch/s, Samples=216, Avg time/batch=0.4235s]\n",
      "Running inference:  12%|█▏        | 27/234 [00:14<01:24,  2.44batch/s, Samples=224, Avg time/batch=0.4203s]\n",
      "Running inference:  12%|█▏        | 28/234 [00:14<01:23,  2.47batch/s, Samples=224, Avg time/batch=0.4203s]\n",
      "Running inference:  12%|█▏        | 28/234 [00:15<01:23,  2.47batch/s, Samples=232, Avg time/batch=0.4184s]\n",
      "Running inference:  12%|█▏        | 29/234 [00:15<01:24,  2.41batch/s, Samples=232, Avg time/batch=0.4184s]\n",
      "Running inference:  12%|█▏        | 29/234 [00:15<01:24,  2.41batch/s, Samples=240, Avg time/batch=0.4165s]\n",
      "Running inference:  13%|█▎        | 30/234 [00:15<01:24,  2.41batch/s, Samples=240, Avg time/batch=0.4165s]\n",
      "Running inference:  13%|█▎        | 30/234 [00:15<01:24,  2.41batch/s, Samples=248, Avg time/batch=0.4144s]\n",
      "Running inference:  13%|█▎        | 31/234 [00:15<01:24,  2.41batch/s, Samples=248, Avg time/batch=0.4144s]\n",
      "Running inference:  13%|█▎        | 31/234 [00:16<01:24,  2.41batch/s, Samples=256, Avg time/batch=0.4126s]\n",
      "Running inference:  14%|█▎        | 32/234 [00:16<01:23,  2.41batch/s, Samples=256, Avg time/batch=0.4126s]\n",
      "Running inference:  14%|█▎        | 32/234 [00:16<01:23,  2.41batch/s, Samples=264, Avg time/batch=0.4087s]\n",
      "Running inference:  14%|█▍        | 33/234 [00:16<01:17,  2.60batch/s, Samples=264, Avg time/batch=0.4087s]\n",
      "Running inference:  14%|█▍        | 33/234 [00:17<01:17,  2.60batch/s, Samples=272, Avg time/batch=0.4071s]\n",
      "Running inference:  15%|█▍        | 34/234 [00:17<01:18,  2.55batch/s, Samples=272, Avg time/batch=0.4071s]\n",
      "Running inference:  15%|█▍        | 34/234 [00:17<01:18,  2.55batch/s, Samples=280, Avg time/batch=0.4051s]\n",
      "Running inference:  15%|█▍        | 35/234 [00:17<01:18,  2.54batch/s, Samples=280, Avg time/batch=0.4051s]\n",
      "Running inference:  15%|█▍        | 35/234 [00:17<01:18,  2.54batch/s, Samples=288, Avg time/batch=0.4052s]\n",
      "Running inference:  15%|█▌        | 36/234 [00:17<01:22,  2.41batch/s, Samples=288, Avg time/batch=0.4052s]\n",
      "Running inference:  15%|█▌        | 36/234 [00:18<01:22,  2.41batch/s, Samples=296, Avg time/batch=0.4040s]\n",
      "Running inference:  16%|█▌        | 37/234 [00:18<01:22,  2.40batch/s, Samples=296, Avg time/batch=0.4040s]\n",
      "Running inference:  16%|█▌        | 37/234 [00:18<01:22,  2.40batch/s, Samples=304, Avg time/batch=0.4023s]\n",
      "Running inference:  16%|█▌        | 38/234 [00:18<01:20,  2.43batch/s, Samples=304, Avg time/batch=0.4023s]\n",
      "Running inference:  16%|█▌        | 38/234 [00:19<01:20,  2.43batch/s, Samples=312, Avg time/batch=0.4006s]\n",
      "Running inference:  17%|█▋        | 39/234 [00:19<01:19,  2.47batch/s, Samples=312, Avg time/batch=0.4006s]\n",
      "Running inference:  17%|█▋        | 39/234 [00:19<01:19,  2.47batch/s, Samples=320, Avg time/batch=0.4004s]\n",
      "Running inference:  17%|█▋        | 40/234 [00:19<01:21,  2.39batch/s, Samples=320, Avg time/batch=0.4004s]\n",
      "Running inference:  17%|█▋        | 40/234 [00:19<01:21,  2.39batch/s, Samples=328, Avg time/batch=0.3994s]\n",
      "Running inference:  18%|█▊        | 41/234 [00:19<01:20,  2.39batch/s, Samples=328, Avg time/batch=0.3994s]\n",
      "Running inference:  18%|█▊        | 41/234 [00:20<01:20,  2.39batch/s, Samples=336, Avg time/batch=0.3984s]\n",
      "Running inference:  18%|█▊        | 42/234 [00:20<01:20,  2.39batch/s, Samples=336, Avg time/batch=0.3984s]\n",
      "Running inference:  18%|█▊        | 42/234 [00:20<01:20,  2.39batch/s, Samples=344, Avg time/batch=0.3983s]\n",
      "Running inference:  18%|█▊        | 43/234 [00:20<01:21,  2.34batch/s, Samples=344, Avg time/batch=0.3983s]\n",
      "Running inference:  18%|█▊        | 43/234 [00:21<01:21,  2.34batch/s, Samples=352, Avg time/batch=0.3980s]\n",
      "Running inference:  19%|█▉        | 44/234 [00:21<01:23,  2.28batch/s, Samples=352, Avg time/batch=0.3980s]\n",
      "Running inference:  19%|█▉        | 44/234 [00:21<01:23,  2.28batch/s, Samples=360, Avg time/batch=0.3978s]\n",
      "Running inference:  19%|█▉        | 45/234 [00:21<01:23,  2.27batch/s, Samples=360, Avg time/batch=0.3978s]\n",
      "Running inference:  19%|█▉        | 45/234 [00:22<01:23,  2.27batch/s, Samples=368, Avg time/batch=0.3976s]\n",
      "Running inference:  20%|█▉        | 46/234 [00:22<01:23,  2.26batch/s, Samples=368, Avg time/batch=0.3976s]\n",
      "Running inference:  20%|█▉        | 46/234 [00:22<01:23,  2.26batch/s, Samples=376, Avg time/batch=0.3969s]\n",
      "Running inference:  20%|██        | 47/234 [00:22<01:21,  2.30batch/s, Samples=376, Avg time/batch=0.3969s]\n",
      "Running inference:  20%|██        | 47/234 [00:22<01:21,  2.30batch/s, Samples=384, Avg time/batch=0.3953s]\n",
      "Running inference:  21%|██        | 48/234 [00:22<01:17,  2.39batch/s, Samples=384, Avg time/batch=0.3953s]\n",
      "Running inference:  21%|██        | 48/234 [00:23<01:17,  2.39batch/s, Samples=392, Avg time/batch=0.3947s]\n",
      "Running inference:  21%|██        | 49/234 [00:23<01:17,  2.38batch/s, Samples=392, Avg time/batch=0.3947s]\n",
      "Running inference:  21%|██        | 49/234 [00:23<01:17,  2.38batch/s, Samples=400, Avg time/batch=0.3946s]\n",
      "Running inference:  21%|██▏       | 50/234 [00:23<01:18,  2.34batch/s, Samples=400, Avg time/batch=0.3946s]\n",
      "Running inference:  21%|██▏       | 50/234 [00:24<01:18,  2.34batch/s, Samples=408, Avg time/batch=0.3941s]\n",
      "Running inference:  22%|██▏       | 51/234 [00:24<01:18,  2.34batch/s, Samples=408, Avg time/batch=0.3941s]\n",
      "Running inference:  22%|██▏       | 51/234 [00:24<01:18,  2.34batch/s, Samples=416, Avg time/batch=0.3941s]\n",
      "Running inference:  22%|██▏       | 52/234 [00:24<01:19,  2.30batch/s, Samples=416, Avg time/batch=0.3941s]\n",
      "Running inference:  22%|██▏       | 52/234 [00:25<01:19,  2.30batch/s, Samples=424, Avg time/batch=0.3930s]\n",
      "Running inference:  23%|██▎       | 53/234 [00:25<01:16,  2.36batch/s, Samples=424, Avg time/batch=0.3930s]\n",
      "Running inference:  23%|██▎       | 53/234 [00:25<01:16,  2.36batch/s, Samples=432, Avg time/batch=0.3929s]\n",
      "Running inference:  23%|██▎       | 54/234 [00:25<01:17,  2.33batch/s, Samples=432, Avg time/batch=0.3929s]\n",
      "Running inference:  23%|██▎       | 54/234 [00:25<01:17,  2.33batch/s, Samples=440, Avg time/batch=0.3928s]\n",
      "Running inference:  24%|██▎       | 55/234 [00:25<01:17,  2.31batch/s, Samples=440, Avg time/batch=0.3928s]\n",
      "Running inference:  24%|██▎       | 55/234 [00:26<01:17,  2.31batch/s, Samples=448, Avg time/batch=0.3923s]\n",
      "Running inference:  24%|██▍       | 56/234 [00:26<01:16,  2.32batch/s, Samples=448, Avg time/batch=0.3923s]\n",
      "Running inference:  24%|██▍       | 56/234 [00:26<01:16,  2.32batch/s, Samples=456, Avg time/batch=0.3919s]\n",
      "Running inference:  24%|██▍       | 57/234 [00:26<01:15,  2.34batch/s, Samples=456, Avg time/batch=0.3919s]\n",
      "Running inference:  24%|██▍       | 57/234 [00:27<01:15,  2.34batch/s, Samples=464, Avg time/batch=0.3909s]\n",
      "Running inference:  25%|██▍       | 58/234 [00:27<01:13,  2.41batch/s, Samples=464, Avg time/batch=0.3909s]\n",
      "Running inference:  25%|██▍       | 58/234 [00:27<01:13,  2.41batch/s, Samples=472, Avg time/batch=0.3902s]\n",
      "Running inference:  25%|██▌       | 59/234 [00:27<01:12,  2.42batch/s, Samples=472, Avg time/batch=0.3902s]\n",
      "Running inference:  25%|██▌       | 59/234 [00:28<01:12,  2.42batch/s, Samples=480, Avg time/batch=0.3889s]\n",
      "Running inference:  26%|██▌       | 60/234 [00:28<01:09,  2.49batch/s, Samples=480, Avg time/batch=0.3889s]\n",
      "Running inference:  26%|██▌       | 60/234 [00:28<01:09,  2.49batch/s, Samples=488, Avg time/batch=0.3888s]\n",
      "Running inference:  26%|██▌       | 61/234 [00:28<01:11,  2.43batch/s, Samples=488, Avg time/batch=0.3888s]\n",
      "Running inference:  26%|██▌       | 61/234 [00:28<01:11,  2.43batch/s, Samples=496, Avg time/batch=0.3889s]\n",
      "Running inference:  26%|██▋       | 62/234 [00:28<01:11,  2.40batch/s, Samples=496, Avg time/batch=0.3889s]\n",
      "Running inference:  26%|██▋       | 62/234 [00:29<01:11,  2.40batch/s, Samples=504, Avg time/batch=0.3891s]\n",
      "Running inference:  27%|██▋       | 63/234 [00:29<01:13,  2.31batch/s, Samples=504, Avg time/batch=0.3891s]\n",
      "Running inference:  27%|██▋       | 63/234 [00:29<01:13,  2.31batch/s, Samples=512, Avg time/batch=0.3889s]\n",
      "Running inference:  27%|██▋       | 64/234 [00:29<01:13,  2.30batch/s, Samples=512, Avg time/batch=0.3889s]\n",
      "Running inference:  27%|██▋       | 64/234 [00:30<01:13,  2.30batch/s, Samples=520, Avg time/batch=0.3884s]\n",
      "Running inference:  28%|██▊       | 65/234 [00:30<01:12,  2.33batch/s, Samples=520, Avg time/batch=0.3884s]\n",
      "Running inference:  28%|██▊       | 65/234 [00:30<01:12,  2.33batch/s, Samples=528, Avg time/batch=0.3887s]\n",
      "Running inference:  28%|██▊       | 66/234 [00:30<01:14,  2.27batch/s, Samples=528, Avg time/batch=0.3887s]\n",
      "Running inference:  28%|██▊       | 66/234 [00:31<01:14,  2.27batch/s, Samples=536, Avg time/batch=0.3879s]\n",
      "Running inference:  29%|██▊       | 67/234 [00:31<01:10,  2.38batch/s, Samples=536, Avg time/batch=0.3879s]\n",
      "Running inference:  29%|██▊       | 67/234 [00:31<01:10,  2.38batch/s, Samples=544, Avg time/batch=0.3877s]\n",
      "Running inference:  29%|██▉       | 68/234 [00:31<01:10,  2.35batch/s, Samples=544, Avg time/batch=0.3877s]\n",
      "Running inference:  29%|██▉       | 68/234 [00:31<01:10,  2.35batch/s, Samples=552, Avg time/batch=0.3871s]\n",
      "Running inference:  29%|██▉       | 69/234 [00:31<01:09,  2.39batch/s, Samples=552, Avg time/batch=0.3871s]\n",
      "Running inference:  29%|██▉       | 69/234 [00:32<01:09,  2.39batch/s, Samples=560, Avg time/batch=0.3867s]\n",
      "Running inference:  30%|██▉       | 70/234 [00:32<01:08,  2.40batch/s, Samples=560, Avg time/batch=0.3867s]\n",
      "Running inference:  30%|██▉       | 70/234 [00:32<01:08,  2.40batch/s, Samples=568, Avg time/batch=0.3867s]\n",
      "Running inference:  30%|███       | 71/234 [00:32<01:09,  2.35batch/s, Samples=568, Avg time/batch=0.3867s]\n",
      "Running inference:  30%|███       | 71/234 [00:33<01:09,  2.35batch/s, Samples=576, Avg time/batch=0.3866s]\n",
      "Running inference:  31%|███       | 72/234 [00:33<01:09,  2.34batch/s, Samples=576, Avg time/batch=0.3866s]\n",
      "Running inference:  31%|███       | 72/234 [00:33<01:09,  2.34batch/s, Samples=584, Avg time/batch=0.3867s]\n",
      "Running inference:  31%|███       | 73/234 [00:33<01:10,  2.29batch/s, Samples=584, Avg time/batch=0.3867s]\n",
      "Running inference:  31%|███       | 73/234 [00:34<01:10,  2.29batch/s, Samples=592, Avg time/batch=0.3863s]\n",
      "Running inference:  32%|███▏      | 74/234 [00:34<01:08,  2.33batch/s, Samples=592, Avg time/batch=0.3863s]\n",
      "Running inference:  32%|███▏      | 74/234 [00:34<01:08,  2.33batch/s, Samples=600, Avg time/batch=0.3859s]\n",
      "Running inference:  32%|███▏      | 75/234 [00:34<01:07,  2.37batch/s, Samples=600, Avg time/batch=0.3859s]\n",
      "Running inference:  32%|███▏      | 75/234 [00:34<01:07,  2.37batch/s, Samples=608, Avg time/batch=0.3859s]\n",
      "Running inference:  32%|███▏      | 76/234 [00:34<01:08,  2.30batch/s, Samples=608, Avg time/batch=0.3859s]\n",
      "Running inference:  32%|███▏      | 76/234 [00:35<01:08,  2.30batch/s, Samples=616, Avg time/batch=0.3856s]\n",
      "Running inference:  33%|███▎      | 77/234 [00:35<01:07,  2.34batch/s, Samples=616, Avg time/batch=0.3856s]\n",
      "Running inference:  33%|███▎      | 77/234 [00:35<01:07,  2.34batch/s, Samples=624, Avg time/batch=0.3850s]\n",
      "Running inference:  33%|███▎      | 78/234 [00:35<01:05,  2.38batch/s, Samples=624, Avg time/batch=0.3850s]\n",
      "Running inference:  33%|███▎      | 78/234 [00:36<01:05,  2.38batch/s, Samples=632, Avg time/batch=0.3848s]\n",
      "Running inference:  34%|███▍      | 79/234 [00:36<01:06,  2.33batch/s, Samples=632, Avg time/batch=0.3848s]\n",
      "Running inference:  34%|███▍      | 79/234 [00:36<01:06,  2.33batch/s, Samples=640, Avg time/batch=0.3849s]\n",
      "Running inference:  34%|███▍      | 80/234 [00:36<01:06,  2.31batch/s, Samples=640, Avg time/batch=0.3849s]\n",
      "Running inference:  34%|███▍      | 80/234 [00:36<01:06,  2.31batch/s, Samples=648, Avg time/batch=0.3839s]\n",
      "Running inference:  35%|███▍      | 81/234 [00:36<01:02,  2.43batch/s, Samples=648, Avg time/batch=0.3839s]\n",
      "Running inference:  35%|███▍      | 81/234 [00:37<01:02,  2.43batch/s, Samples=656, Avg time/batch=0.3838s]\n",
      "Running inference:  35%|███▌      | 82/234 [00:37<01:02,  2.43batch/s, Samples=656, Avg time/batch=0.3838s]\n",
      "Running inference:  35%|███▌      | 82/234 [00:37<01:02,  2.43batch/s, Samples=664, Avg time/batch=0.3827s]\n",
      "Running inference:  35%|███▌      | 83/234 [00:37<00:58,  2.58batch/s, Samples=664, Avg time/batch=0.3827s]\n",
      "Running inference:  35%|███▌      | 83/234 [00:38<00:58,  2.58batch/s, Samples=672, Avg time/batch=0.3827s]\n",
      "Running inference:  36%|███▌      | 84/234 [00:38<01:00,  2.48batch/s, Samples=672, Avg time/batch=0.3827s]\n",
      "Running inference:  36%|███▌      | 84/234 [00:38<01:00,  2.48batch/s, Samples=680, Avg time/batch=0.3836s]\n",
      "Running inference:  36%|███▋      | 85/234 [00:38<01:04,  2.30batch/s, Samples=680, Avg time/batch=0.3836s]\n",
      "Running inference:  36%|███▋      | 85/234 [00:39<01:04,  2.30batch/s, Samples=688, Avg time/batch=0.3833s]\n",
      "Running inference:  37%|███▋      | 86/234 [00:39<01:05,  2.26batch/s, Samples=688, Avg time/batch=0.3833s]\n",
      "Running inference:  37%|███▋      | 86/234 [00:39<01:05,  2.26batch/s, Samples=696, Avg time/batch=0.3833s]\n",
      "Running inference:  37%|███▋      | 87/234 [00:39<01:04,  2.27batch/s, Samples=696, Avg time/batch=0.3833s]\n",
      "Running inference:  37%|███▋      | 87/234 [00:40<01:04,  2.27batch/s, Samples=704, Avg time/batch=0.3833s]\n",
      "Running inference:  38%|███▊      | 88/234 [00:40<01:04,  2.26batch/s, Samples=704, Avg time/batch=0.3833s]\n",
      "Running inference:  38%|███▊      | 88/234 [00:40<01:04,  2.26batch/s, Samples=712, Avg time/batch=0.3844s]\n",
      "Running inference:  38%|███▊      | 89/234 [00:40<01:09,  2.08batch/s, Samples=712, Avg time/batch=0.3844s]\n",
      "Running inference:  38%|███▊      | 89/234 [00:40<01:09,  2.08batch/s, Samples=720, Avg time/batch=0.3842s]\n",
      "Running inference:  38%|███▊      | 90/234 [00:40<01:06,  2.17batch/s, Samples=720, Avg time/batch=0.3842s]\n",
      "Running inference:  38%|███▊      | 90/234 [00:41<01:06,  2.17batch/s, Samples=728, Avg time/batch=0.3836s]\n",
      "Running inference:  39%|███▉      | 91/234 [00:41<01:02,  2.31batch/s, Samples=728, Avg time/batch=0.3836s]\n",
      "Running inference:  39%|███▉      | 91/234 [00:41<01:02,  2.31batch/s, Samples=736, Avg time/batch=0.3845s]\n",
      "Running inference:  39%|███▉      | 92/234 [00:41<01:06,  2.14batch/s, Samples=736, Avg time/batch=0.3845s]\n",
      "Running inference:  39%|███▉      | 92/234 [00:42<01:06,  2.14batch/s, Samples=744, Avg time/batch=0.3841s]\n",
      "Running inference:  40%|███▉      | 93/234 [00:42<01:03,  2.23batch/s, Samples=744, Avg time/batch=0.3841s]\n",
      "Running inference:  40%|███▉      | 93/234 [00:42<01:03,  2.23batch/s, Samples=752, Avg time/batch=0.3849s]\n",
      "Running inference:  40%|████      | 94/234 [00:42<01:05,  2.14batch/s, Samples=752, Avg time/batch=0.3849s]\n",
      "Running inference:  40%|████      | 94/234 [00:43<01:05,  2.14batch/s, Samples=760, Avg time/batch=0.3847s]\n",
      "Running inference:  41%|████      | 95/234 [00:43<01:03,  2.20batch/s, Samples=760, Avg time/batch=0.3847s]\n",
      "Running inference:  41%|████      | 95/234 [00:43<01:03,  2.20batch/s, Samples=768, Avg time/batch=0.3851s]\n",
      "Running inference:  41%|████      | 96/234 [00:43<01:03,  2.18batch/s, Samples=768, Avg time/batch=0.3851s]\n",
      "Running inference:  41%|████      | 96/234 [00:44<01:03,  2.18batch/s, Samples=776, Avg time/batch=0.3847s]\n",
      "Running inference:  41%|████▏     | 97/234 [00:44<01:00,  2.27batch/s, Samples=776, Avg time/batch=0.3847s]\n",
      "Running inference:  41%|████▏     | 97/234 [00:44<01:00,  2.27batch/s, Samples=784, Avg time/batch=0.3847s]\n",
      "Running inference:  42%|████▏     | 98/234 [00:44<01:00,  2.26batch/s, Samples=784, Avg time/batch=0.3847s]\n",
      "Running inference:  42%|████▏     | 98/234 [00:45<01:00,  2.26batch/s, Samples=792, Avg time/batch=0.3850s]\n",
      "Running inference:  42%|████▏     | 99/234 [00:45<01:01,  2.19batch/s, Samples=792, Avg time/batch=0.3850s]\n",
      "Running inference:  42%|████▏     | 99/234 [00:45<01:01,  2.19batch/s, Samples=800, Avg time/batch=0.3851s]\n",
      "Running inference:  43%|████▎     | 100/234 [00:45<01:00,  2.20batch/s, Samples=800, Avg time/batch=0.3851s]\n",
      "Running inference:  43%|████▎     | 100/234 [00:45<01:00,  2.20batch/s, Samples=808, Avg time/batch=0.3847s]\n",
      "Running inference:  43%|████▎     | 101/234 [00:45<00:58,  2.26batch/s, Samples=808, Avg time/batch=0.3847s]\n",
      "Running inference:  43%|████▎     | 101/234 [00:46<00:58,  2.26batch/s, Samples=816, Avg time/batch=0.3856s]\n",
      "Running inference:  44%|████▎     | 102/234 [00:46<01:01,  2.15batch/s, Samples=816, Avg time/batch=0.3856s]\n",
      "Running inference:  44%|████▎     | 102/234 [00:46<01:01,  2.15batch/s, Samples=824, Avg time/batch=0.3855s]\n",
      "Running inference:  44%|████▍     | 103/234 [00:46<01:00,  2.16batch/s, Samples=824, Avg time/batch=0.3855s]\n",
      "Running inference:  44%|████▍     | 103/234 [00:47<01:00,  2.16batch/s, Samples=832, Avg time/batch=0.3853s]\n",
      "Running inference:  44%|████▍     | 104/234 [00:47<00:59,  2.18batch/s, Samples=832, Avg time/batch=0.3853s]\n",
      "Running inference:  44%|████▍     | 104/234 [00:47<00:59,  2.18batch/s, Samples=840, Avg time/batch=0.3848s]\n",
      "Running inference:  45%|████▍     | 105/234 [00:47<00:55,  2.31batch/s, Samples=840, Avg time/batch=0.3848s]\n",
      "Running inference:  45%|████▍     | 105/234 [00:48<00:55,  2.31batch/s, Samples=848, Avg time/batch=0.3848s]\n",
      "Running inference:  45%|████▌     | 106/234 [00:48<00:58,  2.18batch/s, Samples=848, Avg time/batch=0.3848s]\n",
      "Running inference:  45%|████▌     | 106/234 [00:48<00:58,  2.18batch/s, Samples=856, Avg time/batch=0.3850s]\n",
      "Running inference:  46%|████▌     | 107/234 [00:48<00:57,  2.19batch/s, Samples=856, Avg time/batch=0.3850s]\n",
      "Running inference:  46%|████▌     | 107/234 [00:49<00:57,  2.19batch/s, Samples=864, Avg time/batch=0.3845s]\n",
      "Running inference:  46%|████▌     | 108/234 [00:49<00:53,  2.35batch/s, Samples=864, Avg time/batch=0.3845s]\n",
      "Running inference:  46%|████▌     | 108/234 [00:49<00:53,  2.35batch/s, Samples=872, Avg time/batch=0.3854s]\n",
      "Running inference:  47%|████▋     | 109/234 [00:49<00:59,  2.12batch/s, Samples=872, Avg time/batch=0.3854s]\n",
      "Running inference:  47%|████▋     | 109/234 [00:50<00:59,  2.12batch/s, Samples=880, Avg time/batch=0.3860s]\n",
      "Running inference:  47%|████▋     | 110/234 [00:50<00:59,  2.07batch/s, Samples=880, Avg time/batch=0.3860s]\n",
      "Running inference:  47%|████▋     | 110/234 [00:50<00:59,  2.07batch/s, Samples=888, Avg time/batch=0.3854s]\n",
      "Running inference:  47%|████▋     | 111/234 [00:50<00:55,  2.22batch/s, Samples=888, Avg time/batch=0.3854s]\n",
      "Running inference:  47%|████▋     | 111/234 [00:50<00:55,  2.22batch/s, Samples=896, Avg time/batch=0.3843s]\n",
      "Running inference:  48%|████▊     | 112/234 [00:50<00:49,  2.49batch/s, Samples=896, Avg time/batch=0.3843s]\n",
      "Running inference:  48%|████▊     | 112/234 [00:51<00:49,  2.49batch/s, Samples=904, Avg time/batch=0.3838s]\n",
      "Running inference:  48%|████▊     | 113/234 [00:51<00:48,  2.51batch/s, Samples=904, Avg time/batch=0.3838s]\n",
      "Running inference:  48%|████▊     | 113/234 [00:51<00:48,  2.51batch/s, Samples=912, Avg time/batch=0.3851s]\n",
      "Running inference:  49%|████▊     | 114/234 [00:51<00:54,  2.20batch/s, Samples=912, Avg time/batch=0.3851s]\n",
      "Running inference:  49%|████▊     | 114/234 [00:52<00:54,  2.20batch/s, Samples=920, Avg time/batch=0.3849s]\n",
      "Running inference:  49%|████▉     | 115/234 [00:52<00:53,  2.24batch/s, Samples=920, Avg time/batch=0.3849s]\n",
      "Running inference:  49%|████▉     | 115/234 [00:52<00:53,  2.24batch/s, Samples=928, Avg time/batch=0.3848s]\n",
      "Running inference:  50%|████▉     | 116/234 [00:52<00:52,  2.23batch/s, Samples=928, Avg time/batch=0.3848s]\n",
      "Running inference:  50%|████▉     | 116/234 [00:53<00:52,  2.23batch/s, Samples=936, Avg time/batch=0.3848s]\n",
      "Running inference:  50%|█████     | 117/234 [00:53<00:51,  2.25batch/s, Samples=936, Avg time/batch=0.3848s]\n",
      "Running inference:  50%|█████     | 117/234 [00:53<00:51,  2.25batch/s, Samples=944, Avg time/batch=0.3845s]\n",
      "Running inference:  50%|█████     | 118/234 [00:53<00:50,  2.32batch/s, Samples=944, Avg time/batch=0.3845s]\n",
      "Running inference:  50%|█████     | 118/234 [00:53<00:50,  2.32batch/s, Samples=952, Avg time/batch=0.3845s]\n",
      "Running inference:  51%|█████     | 119/234 [00:53<00:50,  2.29batch/s, Samples=952, Avg time/batch=0.3845s]\n",
      "Running inference:  51%|█████     | 119/234 [00:54<00:50,  2.29batch/s, Samples=960, Avg time/batch=0.3845s]\n",
      "Running inference:  51%|█████▏    | 120/234 [00:54<00:49,  2.33batch/s, Samples=960, Avg time/batch=0.3845s]\n",
      "Running inference:  51%|█████▏    | 120/234 [00:54<00:49,  2.33batch/s, Samples=968, Avg time/batch=0.3844s]\n",
      "Running inference:  52%|█████▏    | 121/234 [00:54<00:48,  2.32batch/s, Samples=968, Avg time/batch=0.3844s]\n",
      "Running inference:  52%|█████▏    | 121/234 [00:55<00:48,  2.32batch/s, Samples=976, Avg time/batch=0.3838s]\n",
      "Running inference:  52%|█████▏    | 122/234 [00:55<00:45,  2.45batch/s, Samples=976, Avg time/batch=0.3838s]\n",
      "Running inference:  52%|█████▏    | 122/234 [00:55<00:45,  2.45batch/s, Samples=984, Avg time/batch=0.3831s]\n",
      "Running inference:  53%|█████▎    | 123/234 [00:55<00:43,  2.54batch/s, Samples=984, Avg time/batch=0.3831s]\n",
      "Running inference:  53%|█████▎    | 123/234 [00:55<00:43,  2.54batch/s, Samples=992, Avg time/batch=0.3824s]\n",
      "Running inference:  53%|█████▎    | 124/234 [00:55<00:42,  2.58batch/s, Samples=992, Avg time/batch=0.3824s]\n",
      "Running inference:  53%|█████▎    | 124/234 [00:56<00:42,  2.58batch/s, Samples=1000, Avg time/batch=0.3823s]\n",
      "Running inference:  53%|█████▎    | 125/234 [00:56<00:43,  2.51batch/s, Samples=1000, Avg time/batch=0.3823s]\n",
      "Running inference:  53%|█████▎    | 125/234 [00:56<00:43,  2.51batch/s, Samples=1008, Avg time/batch=0.3827s]\n",
      "Running inference:  54%|█████▍    | 126/234 [00:56<00:47,  2.28batch/s, Samples=1008, Avg time/batch=0.3827s]\n",
      "Running inference:  54%|█████▍    | 126/234 [00:57<00:47,  2.28batch/s, Samples=1016, Avg time/batch=0.3819s]\n",
      "Running inference:  54%|█████▍    | 127/234 [00:57<00:44,  2.42batch/s, Samples=1016, Avg time/batch=0.3819s]\n",
      "Running inference:  54%|█████▍    | 127/234 [00:57<00:44,  2.42batch/s, Samples=1024, Avg time/batch=0.3812s]\n",
      "Running inference:  55%|█████▍    | 128/234 [00:57<00:41,  2.57batch/s, Samples=1024, Avg time/batch=0.3812s]\n",
      "Running inference:  55%|█████▍    | 128/234 [00:57<00:41,  2.57batch/s, Samples=1032, Avg time/batch=0.3803s]\n",
      "Running inference:  55%|█████▌    | 129/234 [00:57<00:37,  2.78batch/s, Samples=1032, Avg time/batch=0.3803s]\n",
      "Running inference:  55%|█████▌    | 129/234 [00:58<00:37,  2.78batch/s, Samples=1040, Avg time/batch=0.3801s]\n",
      "Running inference:  56%|█████▌    | 130/234 [00:58<00:38,  2.68batch/s, Samples=1040, Avg time/batch=0.3801s]\n",
      "Running inference:  56%|█████▌    | 130/234 [00:58<00:38,  2.68batch/s, Samples=1048, Avg time/batch=0.3803s]\n",
      "Running inference:  56%|█████▌    | 131/234 [00:58<00:43,  2.38batch/s, Samples=1048, Avg time/batch=0.3803s]\n",
      "Running inference:  56%|█████▌    | 131/234 [00:59<00:43,  2.38batch/s, Samples=1056, Avg time/batch=0.3806s]\n",
      "Running inference:  56%|█████▋    | 132/234 [00:59<00:44,  2.27batch/s, Samples=1056, Avg time/batch=0.3806s]\n",
      "Running inference:  56%|█████▋    | 132/234 [00:59<00:44,  2.27batch/s, Samples=1064, Avg time/batch=0.3809s]\n",
      "Running inference:  57%|█████▋    | 133/234 [00:59<00:47,  2.15batch/s, Samples=1064, Avg time/batch=0.3809s]\n",
      "Running inference:  57%|█████▋    | 133/234 [01:00<00:47,  2.15batch/s, Samples=1072, Avg time/batch=0.3813s]\n",
      "Running inference:  57%|█████▋    | 134/234 [01:00<00:47,  2.11batch/s, Samples=1072, Avg time/batch=0.3813s]\n",
      "Running inference:  57%|█████▋    | 134/234 [01:00<00:47,  2.11batch/s, Samples=1080, Avg time/batch=0.3807s]\n",
      "Running inference:  58%|█████▊    | 135/234 [01:00<00:42,  2.32batch/s, Samples=1080, Avg time/batch=0.3807s]\n",
      "Running inference:  58%|█████▊    | 135/234 [01:00<00:42,  2.32batch/s, Samples=1088, Avg time/batch=0.3798s]\n",
      "Running inference:  58%|█████▊    | 136/234 [01:00<00:38,  2.56batch/s, Samples=1088, Avg time/batch=0.3798s]\n",
      "Running inference:  58%|█████▊    | 136/234 [01:01<00:38,  2.56batch/s, Samples=1096, Avg time/batch=0.3792s]\n",
      "Running inference:  59%|█████▊    | 137/234 [01:01<00:36,  2.69batch/s, Samples=1096, Avg time/batch=0.3792s]\n",
      "Running inference:  59%|█████▊    | 137/234 [01:01<00:36,  2.69batch/s, Samples=1104, Avg time/batch=0.3791s]\n",
      "Running inference:  59%|█████▉    | 138/234 [01:01<00:37,  2.59batch/s, Samples=1104, Avg time/batch=0.3791s]\n",
      "Running inference:  59%|█████▉    | 138/234 [01:01<00:37,  2.59batch/s, Samples=1112, Avg time/batch=0.3785s]\n",
      "Running inference:  59%|█████▉    | 139/234 [01:01<00:35,  2.67batch/s, Samples=1112, Avg time/batch=0.3785s]\n",
      "Running inference:  59%|█████▉    | 139/234 [01:02<00:35,  2.67batch/s, Samples=1120, Avg time/batch=0.3783s]\n",
      "Running inference:  60%|█████▉    | 140/234 [01:02<00:35,  2.62batch/s, Samples=1120, Avg time/batch=0.3783s]\n",
      "Running inference:  60%|█████▉    | 140/234 [01:02<00:35,  2.62batch/s, Samples=1128, Avg time/batch=0.3777s]\n",
      "Running inference:  60%|██████    | 141/234 [01:02<00:34,  2.67batch/s, Samples=1128, Avg time/batch=0.3777s]\n",
      "Running inference:  60%|██████    | 141/234 [01:03<00:34,  2.67batch/s, Samples=1136, Avg time/batch=0.3781s]\n",
      "Running inference:  61%|██████    | 142/234 [01:03<00:37,  2.44batch/s, Samples=1136, Avg time/batch=0.3781s]\n",
      "Running inference:  61%|██████    | 142/234 [01:03<00:37,  2.44batch/s, Samples=1144, Avg time/batch=0.3783s]\n",
      "Running inference:  61%|██████    | 143/234 [01:03<00:38,  2.36batch/s, Samples=1144, Avg time/batch=0.3783s]\n",
      "Running inference:  61%|██████    | 143/234 [01:04<00:38,  2.36batch/s, Samples=1152, Avg time/batch=0.3784s]\n",
      "Running inference:  62%|██████▏   | 144/234 [01:04<00:39,  2.30batch/s, Samples=1152, Avg time/batch=0.3784s]\n",
      "Running inference:  62%|██████▏   | 144/234 [01:04<00:39,  2.30batch/s, Samples=1160, Avg time/batch=0.3785s]\n",
      "Running inference:  62%|██████▏   | 145/234 [01:04<00:38,  2.29batch/s, Samples=1160, Avg time/batch=0.3785s]\n",
      "Running inference:  62%|██████▏   | 145/234 [01:05<00:38,  2.29batch/s, Samples=1168, Avg time/batch=0.3788s]\n",
      "Running inference:  62%|██████▏   | 146/234 [01:05<00:39,  2.23batch/s, Samples=1168, Avg time/batch=0.3788s]\n",
      "Running inference:  62%|██████▏   | 146/234 [01:05<00:39,  2.23batch/s, Samples=1176, Avg time/batch=0.3784s]\n",
      "Running inference:  63%|██████▎   | 147/234 [01:05<00:37,  2.34batch/s, Samples=1176, Avg time/batch=0.3784s]\n",
      "Running inference:  63%|██████▎   | 147/234 [01:05<00:37,  2.34batch/s, Samples=1184, Avg time/batch=0.3785s]\n",
      "Running inference:  63%|██████▎   | 148/234 [01:05<00:37,  2.31batch/s, Samples=1184, Avg time/batch=0.3785s]\n",
      "Running inference:  63%|██████▎   | 148/234 [01:06<00:37,  2.31batch/s, Samples=1192, Avg time/batch=0.3785s]\n",
      "Running inference:  64%|██████▎   | 149/234 [01:06<00:37,  2.28batch/s, Samples=1192, Avg time/batch=0.3785s]\n",
      "Running inference:  64%|██████▎   | 149/234 [01:06<00:37,  2.28batch/s, Samples=1200, Avg time/batch=0.3785s]\n",
      "Running inference:  64%|██████▍   | 150/234 [01:06<00:36,  2.30batch/s, Samples=1200, Avg time/batch=0.3785s]\n",
      "Running inference:  64%|██████▍   | 150/234 [01:07<00:36,  2.30batch/s, Samples=1208, Avg time/batch=0.3789s]\n",
      "Running inference:  65%|██████▍   | 151/234 [01:07<00:37,  2.21batch/s, Samples=1208, Avg time/batch=0.3789s]\n",
      "Running inference:  65%|██████▍   | 151/234 [01:07<00:37,  2.21batch/s, Samples=1216, Avg time/batch=0.3792s]\n",
      "Running inference:  65%|██████▍   | 152/234 [01:07<00:38,  2.14batch/s, Samples=1216, Avg time/batch=0.3792s]\n",
      "Running inference:  65%|██████▍   | 152/234 [01:08<00:38,  2.14batch/s, Samples=1224, Avg time/batch=0.3791s]\n",
      "Running inference:  65%|██████▌   | 153/234 [01:08<00:36,  2.22batch/s, Samples=1224, Avg time/batch=0.3791s]\n",
      "Running inference:  65%|██████▌   | 153/234 [01:08<00:36,  2.22batch/s, Samples=1232, Avg time/batch=0.3791s]\n",
      "Running inference:  66%|██████▌   | 154/234 [01:08<00:35,  2.24batch/s, Samples=1232, Avg time/batch=0.3791s]\n",
      "Running inference:  66%|██████▌   | 154/234 [01:08<00:35,  2.24batch/s, Samples=1240, Avg time/batch=0.3789s]\n",
      "Running inference:  66%|██████▌   | 155/234 [01:08<00:34,  2.30batch/s, Samples=1240, Avg time/batch=0.3789s]\n",
      "Running inference:  66%|██████▌   | 155/234 [01:09<00:34,  2.30batch/s, Samples=1248, Avg time/batch=0.3787s]\n",
      "Running inference:  67%|██████▋   | 156/234 [01:09<00:33,  2.35batch/s, Samples=1248, Avg time/batch=0.3787s]\n",
      "Running inference:  67%|██████▋   | 156/234 [01:09<00:33,  2.35batch/s, Samples=1256, Avg time/batch=0.3789s]\n",
      "Running inference:  67%|██████▋   | 157/234 [01:09<00:33,  2.28batch/s, Samples=1256, Avg time/batch=0.3789s]\n",
      "Running inference:  67%|██████▋   | 157/234 [01:10<00:33,  2.28batch/s, Samples=1264, Avg time/batch=0.3787s]\n",
      "Running inference:  68%|██████▊   | 158/234 [01:10<00:32,  2.35batch/s, Samples=1264, Avg time/batch=0.3787s]\n",
      "Running inference:  68%|██████▊   | 158/234 [01:10<00:32,  2.35batch/s, Samples=1272, Avg time/batch=0.3786s]\n",
      "Running inference:  68%|██████▊   | 159/234 [01:10<00:32,  2.34batch/s, Samples=1272, Avg time/batch=0.3786s]\n",
      "Running inference:  68%|██████▊   | 159/234 [01:11<00:32,  2.34batch/s, Samples=1280, Avg time/batch=0.3790s]\n",
      "Running inference:  68%|██████▊   | 160/234 [01:11<00:33,  2.23batch/s, Samples=1280, Avg time/batch=0.3790s]\n",
      "Running inference:  68%|██████▊   | 160/234 [01:11<00:33,  2.23batch/s, Samples=1288, Avg time/batch=0.3788s]\n",
      "Running inference:  69%|██████▉   | 161/234 [01:11<00:32,  2.23batch/s, Samples=1288, Avg time/batch=0.3788s]\n",
      "Running inference:  69%|██████▉   | 161/234 [01:12<00:32,  2.23batch/s, Samples=1296, Avg time/batch=0.3790s]\n",
      "Running inference:  69%|██████▉   | 162/234 [01:12<00:32,  2.20batch/s, Samples=1296, Avg time/batch=0.3790s]\n",
      "Running inference:  69%|██████▉   | 162/234 [01:12<00:32,  2.20batch/s, Samples=1304, Avg time/batch=0.3789s]\n",
      "Running inference:  70%|██████▉   | 163/234 [01:12<00:31,  2.25batch/s, Samples=1304, Avg time/batch=0.3789s]\n",
      "Running inference:  70%|██████▉   | 163/234 [01:12<00:31,  2.25batch/s, Samples=1312, Avg time/batch=0.3788s]\n",
      "Running inference:  70%|███████   | 164/234 [01:12<00:31,  2.25batch/s, Samples=1312, Avg time/batch=0.3788s]\n",
      "Running inference:  70%|███████   | 164/234 [01:13<00:31,  2.25batch/s, Samples=1320, Avg time/batch=0.3788s]\n",
      "Running inference:  71%|███████   | 165/234 [01:13<00:29,  2.32batch/s, Samples=1320, Avg time/batch=0.3788s]\n",
      "Running inference:  71%|███████   | 165/234 [01:13<00:29,  2.32batch/s, Samples=1328, Avg time/batch=0.3787s]\n",
      "Running inference:  71%|███████   | 166/234 [01:13<00:29,  2.30batch/s, Samples=1328, Avg time/batch=0.3787s]\n",
      "Running inference:  71%|███████   | 166/234 [01:14<00:29,  2.30batch/s, Samples=1336, Avg time/batch=0.3793s]\n",
      "Running inference:  71%|███████▏  | 167/234 [01:14<00:30,  2.17batch/s, Samples=1336, Avg time/batch=0.3793s]\n",
      "Running inference:  71%|███████▏  | 167/234 [01:14<00:30,  2.17batch/s, Samples=1344, Avg time/batch=0.3794s]\n",
      "Running inference:  72%|███████▏  | 168/234 [01:14<00:30,  2.19batch/s, Samples=1344, Avg time/batch=0.3794s]\n",
      "Running inference:  72%|███████▏  | 168/234 [01:15<00:30,  2.19batch/s, Samples=1352, Avg time/batch=0.3796s]\n",
      "Running inference:  72%|███████▏  | 169/234 [01:15<00:30,  2.15batch/s, Samples=1352, Avg time/batch=0.3796s]\n",
      "Running inference:  72%|███████▏  | 169/234 [01:15<00:30,  2.15batch/s, Samples=1360, Avg time/batch=0.3802s]\n",
      "Running inference:  73%|███████▎  | 170/234 [01:15<00:30,  2.07batch/s, Samples=1360, Avg time/batch=0.3802s]\n",
      "Running inference:  73%|███████▎  | 170/234 [01:16<00:30,  2.07batch/s, Samples=1368, Avg time/batch=0.3813s]\n",
      "Running inference:  73%|███████▎  | 171/234 [01:16<00:32,  1.92batch/s, Samples=1368, Avg time/batch=0.3813s]\n",
      "Running inference:  73%|███████▎  | 171/234 [01:16<00:32,  1.92batch/s, Samples=1376, Avg time/batch=0.3814s]\n",
      "Running inference:  74%|███████▎  | 172/234 [01:16<00:31,  1.99batch/s, Samples=1376, Avg time/batch=0.3814s]\n",
      "Running inference:  74%|███████▎  | 172/234 [01:17<00:31,  1.99batch/s, Samples=1384, Avg time/batch=0.3814s]\n",
      "Running inference:  74%|███████▍  | 173/234 [01:17<00:29,  2.06batch/s, Samples=1384, Avg time/batch=0.3814s]\n",
      "Running inference:  74%|███████▍  | 173/234 [01:17<00:29,  2.06batch/s, Samples=1392, Avg time/batch=0.3813s]\n",
      "Running inference:  74%|███████▍  | 174/234 [01:17<00:27,  2.15batch/s, Samples=1392, Avg time/batch=0.3813s]\n",
      "Running inference:  74%|███████▍  | 174/234 [01:18<00:27,  2.15batch/s, Samples=1400, Avg time/batch=0.3822s]\n",
      "Running inference:  75%|███████▍  | 175/234 [01:18<00:29,  2.01batch/s, Samples=1400, Avg time/batch=0.3822s]\n"
     ]
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect\n",
    "if 'run_inference' in locals() and run_inference:\n",
    "    print(\"Starte Inference-Prozess...\")\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            \"python\", \"inference_subprocess.py\",\n",
    "            \"--user\", USER,\n",
    "            \"--experiment_group\", config[\"data\"][\"experiment_group\"],\n",
    "            \"--experiment_id\", config[\"data\"][\"experiment_id\"],\n",
    "            \"--batch_size\", \"8\",\n",
    "            \"--save_results\"\n",
    "        ], capture_output=True, text=True, check=False)  # check=False um die Exception zu vermeiden\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(\"Fehler beim Ausführen des Inference-Skripts:\")\n",
    "            print(result.stderr)\n",
    "        else:\n",
    "            print(\"Inference erfolgreich abgeschlossen!\")\n",
    "            print(result.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Ausführen des Subprozesses: {e}\")\n",
    "    # try:\n",
    "    #     # Du kannst hier Argumente anpassen\n",
    "    #     result = subprocess.run([\n",
    "    #         \"python\", \"utils/inference_subprocess.py\",\n",
    "    #         \"--user\", USER,\n",
    "    #         \"--experiment_group\", EXPERIMENT_GROUP,\n",
    "    #         \"--experiment_id\", EXPERIMENT_ID,\n",
    "    #         \"--batch_size\", \"8\",\n",
    "    #         \"--save_results\"\n",
    "    #     ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "    #     print(\"Inference erfolgreich abgeschlossen!\")\n",
    "    #     print(result.stdout)\n",
    "        \n",
    "    #     # Suche nach der neuesten Ergebnis-Datei\n",
    "    #     latest_result = max(RESULTS_DIR.glob(\"inference_results_*.pkl\"), key=os.path.getmtime)\n",
    "    #     with open(latest_result, 'rb') as f:\n",
    "    #         results = pickle.load(f)\n",
    "    #     print(f\"Neueste Ergebnisse aus {latest_result} geladen.\")\n",
    "        \n",
    "    # except subprocess.CalledProcessError as e:\n",
    "    #     print(\"Fehler beim Ausführen des Inference-Skripts:\")\n",
    "    #     print(e.stderr)\n",
    "    #     raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance-Metriken anzeigen\n",
    "if 'results' in locals():\n",
    "    perf = results['performance']\n",
    "    print(f\"Anzahl der verarbeiteten Samples: {perf['total_samples']}\")\n",
    "    print(f\"Gesamte Verarbeitungszeit: {perf['total_processing_time']:.2f} Sekunden\")\n",
    "    print(f\"Durchschnittliche Zeit pro Sample: {perf['avg_time_per_sample']:.4f} Sekunden\")\n",
    "    \n",
    "    print(\"\\nPre-Disaster Klassenverteilung:\")\n",
    "    for cls, count in perf['pre_class_distribution'].items():\n",
    "        print(f\"  Klasse {cls}: {count} Pixel\")\n",
    "    \n",
    "    print(\"\\nPost-Disaster Klassenverteilung:\")\n",
    "    for cls, count in perf['post_class_distribution'].items():\n",
    "        print(f\"  Klasse {cls}: {count} Pixel\")\n",
    "\n",
    "# %%\n",
    "# Visualisierungsfunktion importieren und ausführen\n",
    "from visualize import visualize_predictions  # Passe an deine Importstruktur an\n",
    "\n",
    "# Visualisiere eine bestimmte Anzahl von Samples\n",
    "num_samples_to_visualize = 5\n",
    "save_dir = RESULTS_DIR / \"visualizations\"\n",
    "visualize_predictions(results, num_samples=num_samples_to_visualize, save_dir=save_dir)\n",
    "\n",
    "# Zeige einige der generierten Bilder im Notebook an\n",
    "visualizations = list(save_dir.glob(\"prediction_*.png\"))\n",
    "for vis_file in visualizations[:3]:  # Zeige die ersten 3 Bilder\n",
    "    display(HTML(f'<img src=\"{vis_file}\" width=\"800\">'))\n",
    "\n",
    "# Zeige auch die Klassenverteilung an\n",
    "display(HTML(f'<img src=\"{save_dir}/class_distribution.png\" width=\"800\">'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

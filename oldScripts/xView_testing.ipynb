{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import torchvision.transforms.v2 as v2\n",
    "from typing import Optional, Tuple, Dict, List, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HPC Terrabyte\n",
    "# # adapt the user to your needs\n",
    "# USER = \"di97ren\"\n",
    "# # keep the following unchanged\n",
    "# ROOT = Path(\"/dss/dsstbyfs02/pn49ci/pn49ci-dss-0022\")\n",
    "# USER_PATH = ROOT / f\"users/{USER}\"\n",
    "# DATA_PATH = ROOT / \"data\"\n",
    "\n",
    "# # when you are on a local dev client\n",
    "# # uncomment these lines and make necessary ajdustments\n",
    "# #ROOT = Path(\"C:/projects/hands-on-DL\")\n",
    "# #DATA_PATH = Path(\"../data\")\n",
    "\n",
    "# # Configure the path to the GWHD dataset for your environment\n",
    "# DATASET_ROOT = DATA_PATH / \"xview2-subset\"\n",
    "\n",
    "# IMG_PATH = DATASET_ROOT / \"png_images\"\n",
    "# TARGET_PATH = DATASET_ROOT / 'targets'\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform():\n",
    "    \"\"\"Transform für Bilder & Masken\"\"\"\n",
    "    return v2.Compose([\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomVerticalFlip(),\n",
    "        v2.RandomRotation(degrees=15),\n",
    "        v2.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "        v2.ToDtype(torch.float32, scale=True)  # Automatische Skalierung auf [0,1]\n",
    "    ])\n",
    "\n",
    "def image_transform():\n",
    "    \"\"\"Nur für RGB-Bilder\"\"\"\n",
    "    return v2.Compose([\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        v2.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "class xView2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 png_path: str,\n",
    "                 target_path: str,\n",
    "                 transform: callable = None,\n",
    "                 image_transform: callable = None):\n",
    "        \n",
    "        self.png_path = png_path\n",
    "        self.target_path = target_path\n",
    "        self.transform = transform\n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "\n",
    "        # get all pre-disaster images:\n",
    "        self.pre_images = sorted(self.png_path.glob(\"*_pre_disaster.png\"))\n",
    "        \n",
    "        self.pairs = [] #\n",
    "\n",
    "        for pre_img_path in self.pre_images:\n",
    "            post_img_path = self.png_path / pre_img_path.name.replace(\"_pre_disaster\", \"_post_disaster\")\n",
    "\n",
    "            post_target_path = self.target_path / pre_img_path.name.replace(\"_pre_disaster\", \"_post_disaster\")\n",
    "            pre_target_path = pre_target_path = self.target_path / pre_img_path.name\n",
    "\n",
    "\n",
    "            if post_img_path.exists() and post_target_path.exists() and pre_target_path.exists():\n",
    "                self.pairs.append((pre_img_path, post_img_path, pre_target_path, post_target_path))\n",
    "\n",
    "        print(f\"Total pairs found: {len(self.pairs)}\")\n",
    "        assert len(self.pairs) > 0, \"No matching image-pairs found!\"\n",
    "\n",
    "\n",
    "\n",
    "        # super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pre_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pre_img_path, post_img_path, pre_target_path, post_target_path = self.pairs[index]\n",
    "\n",
    "        # load images and target masks with \n",
    "        \n",
    "        pre_img = Image.open(pre_img_path).convert(\"RGB\")\n",
    "        post_img = Image.open(post_img_path).convert(\"RGB\")\n",
    "        pre_target_mask = Image.open(pre_target_path).convert('L')\n",
    "        post_target_mask = Image.open(post_target_path).convert('L')\n",
    "\n",
    "        # convert to numpy arrays\n",
    "        pre_img = np.array(pre_img, dtype=np.float32) / 255.0\n",
    "        post_img = np.array(post_img, dtype=np.float32) / 255.0\n",
    "        pre_target_mask = np.array(pre_target_mask, dtype=np.float32)\n",
    "        post_target_mask = np.array(post_target_mask, dtype=np.float32)\n",
    "\n",
    "        # convert to Tensor\n",
    "        pre_img = torch.tensor(pre_img).permute(2, 0, 1)  # (H, W, C) → (C, H, W)\n",
    "        post_img = torch.tensor(post_img).permute(2, 0, 1)\n",
    "        pre_target_mask = torch.tensor(pre_target_mask).unsqueeze(0)  # (H, W) → (1, H, W)\n",
    "        post_target_mask = torch.tensor(post_target_mask).unsqueeze(0)\n",
    "\n",
    "    # Transformation (optional)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            stack = torch.cat([pre_img, post_img, pre_target_mask, post_target_mask], dim=0)  # (8, H, W)\n",
    "            stack = self.transform(stack)\n",
    "\n",
    "            pre_img, post_img, pre_target_mask, post_target_mask = stack[:3], stack[3:6], stack[6:7], stack[7:8]\n",
    "        \n",
    "        if self.image_transform:\n",
    "            \n",
    "            # Nur auf Bilder Normalisierung anwenden\n",
    "            pre_img = self.image_transform(pre_img)\n",
    "            post_img = self.image_transform(post_img)\n",
    "\n",
    "        return pre_img, post_img, pre_target_mask, post_target_mask \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = xView2Dataset(png_path= IMG_PATH,\n",
    "#                         target_path = TARGET_PATH,\n",
    "#                         transform = transform (),\n",
    "#                         image_transform = image_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Extrahiere die Daten aus dem Batch\n",
    "    pre_imgs, post_imgs, pre_masks, post_masks = zip(*batch)\n",
    "\n",
    "    # Staple die Tensoren entlang der Batch-Dimension\n",
    "    pre_imgs = torch.stack(pre_imgs, dim=0)  # [Batch size, Channels, Height, Width]\n",
    "    post_imgs = torch.stack(post_imgs, dim=0)  # [Batch size, Channels, Height, Width]\n",
    "    pre_masks = torch.stack(pre_masks, dim=0)  # [Batch size, Height, Width]\n",
    "    post_masks = torch.stack(post_masks, dim=0)  # [Batch size, Height, Width]\n",
    "\n",
    "    # Wenn Masken ebenfalls als 4D-Tensor erwartet werden (Batch size, 1, Height, Width)\n",
    "    #pre_masks = pre_masks.unsqueeze(1)  # [Batch size, 1, Height, Width]\n",
    "    #post_masks = post_masks.unsqueeze(1)  # [Batch size, 1, Height, Width]\n",
    "\n",
    "    # # Die Eingabe ist ein 4D-Tensor [Batch size, Channels, Height, Width]\n",
    "    images = torch.cat([pre_imgs, post_imgs], dim=0)  # [2 * Batch size, Channels, Height, Width]\n",
    "    \n",
    "    # # Masken als 4D-Tensor [Batch size, 1, Height, Width]\n",
    "    masks = torch.cat([pre_masks, post_masks], dim=0)  # [2 * Batch size, 1, Height, Width]\n",
    "\n",
    "    return {\n",
    "        \"image\": images,  # 4D Tensor\n",
    "        \"masks\": masks,   # 4D Tensor\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     # Extrahiere die Daten aus dem Batch\n",
    "#     pre_imgs, post_imgs, pre_masks, post_masks = zip(*batch)\n",
    "\n",
    "#     # Staple die Tensoren entlang der Batch-Dimension\n",
    "#     pre_imgs = torch.stack(pre_imgs, dim=0)\n",
    "#     post_imgs = torch.stack(post_imgs, dim=0)\n",
    "#     pre_masks = torch.stack(pre_masks, dim=0)\n",
    "#     post_masks = torch.stack(post_masks, dim=0)\n",
    "\n",
    "#     Format für das Modell: {\"image\": [pre_imgs, post_imgs], \"mask\": [pre_masks, post_masks]}\n",
    "#     Damit das Modell beide Masken gleichzeitig verwenden kann\n",
    "#     return {\n",
    "#         \"image\": [pre_imgs, post_imgs],  # Beide Bilder als Eingabe\n",
    "#         \"masks\": [pre_masks, post_masks]  # Beide Masken als Ziel\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=5,\n",
    "#     collate_fn = collate_fn,\n",
    "#     shuffle=True,\n",
    "#     num_workers= 0,\n",
    "#     drop_last=True\n",
    "# )\n",
    "\n",
    "# # Teste die Anzahl der Batches im DataLoader\n",
    "# print(f\"Number of batches in train_loader: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# args = argparse.Namespace(\n",
    "#         train_images_path = IMG_PATH,\n",
    "#         train_masks_path = TARGET_PATH,\n",
    "#         batch_size = 5\n",
    "# )  \n",
    "\n",
    "\n",
    "# training_dataset = create_train_dataloader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "def convert_to_labels(loss_str, logits):\n",
    "    if loss_str == \"mse\":\n",
    "        preds = torch.round(F.relu(logits[:, 0], inplace=True)) + 1\n",
    "        preds[preds > 4] = 4\n",
    "    # elif loss_str == \"coral\":\n",
    "    #     preds = torch.sum(torch.sigmoid(logits) > 0.5, dim=1) + 1\n",
    "    else:\n",
    "        preds = torch.argmax(logits, dim=1) + 1\n",
    "    return preds\n",
    "\n",
    "class F1(torchmetrics.Metric):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.loss_str = args.loss_str\n",
    "        self.n_class = 5\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.f1_metric = F1Score(task = \"multiclass\", num_classes=self.n_class) \n",
    "                                 #,average='macro', mdmc_average='global')\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        probs = self.softmax(preds) if self.loss_str not in \"mse\" else preds\n",
    "        if self.n_class == 5:\n",
    "            preds = convert_to_labels(self.loss_str, probs)\n",
    "            mask = targets > 0\n",
    "            targets = targets[mask]\n",
    "            preds = preds[mask]\n",
    "        else:\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        self.f1_metric.update(preds, targets)\n",
    "\n",
    "    def compute(self):\n",
    "        f1_score = self.f1_metric.compute()\n",
    "        return f1_score.cpu()\n",
    "\n",
    "    def reset(self):\n",
    "        self.f1_metric.reset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model\n",
    "Ich möchte ein Siamese Neural Network trainiern. Es soll UNets nutzen und ResNet50 als Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition: \n",
    "### Dilation:\n",
    "Die Dilation in einer Convolutional-Schicht erweitert das Empfangsfenster des Filters, indem Lücken zwischen den benachbarten Pixeln eingefügt werden. Dadurch wird der Kontext, den der Filter bei der Verarbeitung eines Bildes sieht, vergrößert, ohne die Filtergröße zu erhöhen.\n",
    "\n",
    "Auswirkungen im Code:\n",
    "In deinem U-Net-Code hat die Dilation Auswirkungen auf den Decoder:\n",
    "\n",
    "dilation = 1: Standard-Konvolution (kein Abstand zwischen den Filtern). Der Filter sieht nur benachbarte Pixel.\n",
    "dilation = 2: Der Filter überspringt jedes zweite Pixel und erfasst somit ein größeres Gebiet des Bildes.\n",
    "dilation = 4: Der Filter überspringt jedes vierte Pixel und sieht ein noch größeres Gebiet.\n",
    "Was passiert im Code:\n",
    "Die Dilation beeinflusst, wie die Schichten im Decoder miteinander verbunden sind und wie viel Kontext sie erfassen.\n",
    "Je höher der Dilation-Wert, desto mehr \"Sicht\" hat der Filter auf das Bild und desto mehr Kontext kann er bei der Segmentierung erfassen.\n",
    "Beispiel:\n",
    "Dilation = 1: Filter sieht 3x3 benachbarte Pixel.\n",
    "Dilation = 2: Filter überspringt 1 Pixel zwischen den benachbarten Pixeln (sieht ein 5x5-Feld).\n",
    "Dilation = 4: Filter überspringt 3 Pixel zwischen den benachbarten Pixeln (sieht ein 7x7-Feld)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    \"ce\": nn.CrossEntropyLoss(),\n",
    "    \"mse\": nn.MSELoss(),\n",
    "}\n",
    "\n",
    "# class Loss(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super().__init__()\n",
    "#         self.loss_str = args.loss_str\n",
    "#         self.losses = nn.ModuleList([losses[loss_fn] for loss_fn in self.loss_str.split(\"+\")])\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "#         # Sicherstellen, dass y_true ein Tensor ist\n",
    "#         if not isinstance(y_true, torch.Tensor):  # Wenn y_true kein Tensor ist\n",
    "#             y_true = torch.tensor(y_true, dtype=torch.float32).to(y_pred.device)\n",
    "\n",
    "#         device = y_pred.device\n",
    "#         mask = y_true > 0  # Maskierung basierend auf Tensor-Werten\n",
    "\n",
    "#         # Interpolation\n",
    "#         y_pred = F.interpolate(y_pred, size=y_true.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "#         # Maskierung der Vorhersage und Zielwerte\n",
    "#         y_pred = torch.stack([y_pred[:, i][mask] for i in range(y_pred.shape[1])], 1).to(device)\n",
    "#         y_true = y_true[mask] - 1\n",
    "\n",
    "#         if self.loss_str == \"mse\":\n",
    "#             y_pred = F.relu(y_pred[:, 0], inplace=True)\n",
    "#             y_true = y_true.float()\n",
    "#         else:  # \"ce\"\n",
    "#             y_true = y_true.long()\n",
    "\n",
    "#         # Berechnung des Gesamtverlustes\n",
    "#         loss = 0\n",
    "#         for loss_fn in self.losses:\n",
    "#             loss += loss_fn(y_pred, y_true)\n",
    "#        return loss\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.loss_str = args.loss_str\n",
    "       #  self.post = args.type == \"post\" -> brauche ich nicht\n",
    "        self.losses = nn.ModuleList([losses[loss_fn] for loss_fn in self.loss_str.split(\"+\")])\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # if self.post:\n",
    "        #     device = y_pred.device\n",
    "        #     mask = y_true > 0\n",
    "        #     y_pred = torch.stack([y_pred[:, i][mask] for i in range(y_pred.shape[1])], 1).to(device)\n",
    "        #     y_true = y_true[mask] - 1\n",
    "\n",
    "        # device = y_pred.device\n",
    "        # mask = y_true > 0\n",
    "\n",
    "        # y_pred = torch.stack([y_pred[:, i][mask] for i in range(y_pred.shape[1])], 1).to(device)\n",
    "        # y_pred = F.interpolate(y_pred, size=y_true.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # y_true = y_true[mask] -1\n",
    "        device = y_pred.device\n",
    "        mask = y_true > 0\n",
    "\n",
    "        # Erst y_pred auf die gleiche Größe wie y_true bringen\n",
    "        y_pred = F.interpolate(y_pred, size=y_true.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Dann maskieren\n",
    "        y_pred = torch.stack([y_pred[:, i][mask] for i in range(y_pred.shape[1])], 1).to(device)\n",
    "        y_true = y_true[mask] - 1\n",
    "\n",
    "\n",
    "        if self.loss_str == \"mse\":\n",
    "            y_pred = F.relu(y_pred[:, 0], inplace=True)\n",
    "            y_true = y_true.float()\n",
    "        else: # \"ce\"\n",
    "            y_true = y_true.long()\n",
    "\n",
    "        loss = 0\n",
    "        for loss_fn in self.losses:\n",
    "            loss += loss_fn(y_pred, y_true)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(PPM, self).__init__()\n",
    "        self.features = []\n",
    "        out_channels = in_channels // 4\n",
    "        print(in_channels.size())\n",
    "        for bin in (1, 2, 3, 6):\n",
    "            self.features.append(\n",
    "                nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d(bin),\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(out_channels, affine=True),\n",
    "                    nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "                )\n",
    "            )\n",
    "        self.features = nn.ModuleList(self.features)\n",
    "        self.conv = nn.Conv2d(2 * in_channels, in_channels, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sicherstellen, dass die Eingabe die Form [Batch size, Channels, Height, Width] hat\n",
    "\n",
    "\n",
    "        x_size = x.size()\n",
    "        out = [x]\n",
    "        for f in self.features:\n",
    "            out.append(F.interpolate(f(x), x_size[2:], mode=\"bilinear\", align_corners=True))\n",
    "        out = self.conv(torch.cat(out, 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class ASPPModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, dilation):\n",
    "        super(ASPPModule, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, affine=True)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, dilation):\n",
    "        super(ASPP, self).__init__()\n",
    "        out_channels = in_channels // 4\n",
    "        dilations = [1, 3 * dilation, 6 * dilation, 9 * dilation]\n",
    "        self.aspp1 = ASPPModule(in_channels, out_channels, 1, padding=0, dilation=dilations[0])\n",
    "        self.aspp2 = ASPPModule(in_channels, out_channels, 3, padding=dilations[1], dilation=dilations[1])\n",
    "        self.aspp3 = ASPPModule(in_channels, out_channels, 3, padding=dilations[2], dilation=dilations[2])\n",
    "        self.aspp4 = ASPPModule(in_channels, out_channels, 3, padding=dilations[3], dilation=dilations[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        out = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv(inputs)\n",
    "        out = self.batch_norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels, affine=True)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv(inputs)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.lrelu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FusionBlock(nn.Module):\n",
    "    def __init__(self, pre_conv, post_conv, channels):\n",
    "        super(FusionBlock, self).__init__()\n",
    "        self.pre_conv = pre_conv\n",
    "        self.post_conv = post_conv\n",
    "        self.conv_pre = ConvLayer(2 * channels, channels)\n",
    "        self.conv_post = ConvLayer(2 * channels, channels)\n",
    "\n",
    "    def forward(self, pre, post, dec_pre=None, dec_post=None, last_dec=False):\n",
    "        pre = self.pre_conv(pre, dec_pre) if dec_pre is not None or last_dec else self.pre_conv(pre)\n",
    "        post = self.post_conv(post, dec_post) if dec_post is not None or last_dec else self.post_conv(post)\n",
    "        fmap = torch.cat([pre, post], 1)\n",
    "        pre, post = self.conv_pre(fmap), self.conv_post(fmap)\n",
    "        return pre, post\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(in_channels, out_channels)\n",
    "        self.conv2 = ConvLayer(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.conv2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, attention, dec_interp):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.attention = attention\n",
    "        self.dec_interp = dec_interp\n",
    "        self.skip_channels = skip_channels\n",
    "        inc = skip_channels + out_channels\n",
    "        if self.dec_interp:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
    "        else:\n",
    "            self.conv_tranpose = ConvTranspose(in_channels, out_channels)\n",
    "\n",
    "        self.conv_block = ConvBlock(inc, out_channels)\n",
    "        if skip_channels > 0 and self.attention:\n",
    "            att_out = out_channels // 2\n",
    "            self.conv_o = AttentionLayer(out_channels, att_out)\n",
    "            self.conv_s = AttentionLayer(skip_channels, att_out)\n",
    "            self.psi = AttentionLayer(att_out, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        if self.dec_interp:\n",
    "            out = F.interpolate(self.conv(inputs), scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        else:\n",
    "            out = self.conv_tranpose(inputs)\n",
    "\n",
    "        if self.skip_channels == 0:\n",
    "            return self.conv_block(out)\n",
    "\n",
    "        if self.attention:\n",
    "            out_a = self.conv_o(out)\n",
    "            skip_a = self.conv_s(skip)\n",
    "            psi_a = self.psi(self.relu(out_a + skip_a))\n",
    "            attention = self.sigmoid(psi_a)\n",
    "            skip = skip * attention\n",
    "        out = self.conv_block(torch.cat((out, skip), dim=1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_class, interpolate):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        self.interpolate = interpolate\n",
    "        # self.coral_loss = n_class == 3 -> vermutlich unwichtig, weil ich immer mehr als 3 Klassen habe. \n",
    "        # if self.coral_loss:\n",
    "        #     self.conv = nn.Conv2d(in_channels, 1, kernel_size=1, bias=False)\n",
    "        #     self.bias = nn.Parameter(torch.tensor([[[1.0]], [[0.0]], [[-1.0]]]))\n",
    "        # else:\n",
    "        #     self.conv = nn.Conv2d(in_channels, n_class, kernel_size=1)\n",
    "        self.conv = nn.Conv2d(in_channels, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv(inputs)\n",
    "        # if self.coral_loss:\n",
    "        #     out = out + self.bias\n",
    "        if self.interpolate:\n",
    "            size = (512, 512) if self.training else (1024, 1024)\n",
    "            out = F.interpolate(out, size, mode=\"bilinear\", align_corners=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_encoder(encoder_str, dilation, pretrained=True, in_channels=3):\n",
    "    assert \"resnet\" in encoder_str \n",
    "\n",
    "   \n",
    "    encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "    replace_stride_with_dilation = [False, dilation == 4, dilation in [2, 4]]\n",
    "\n",
    "    # ResNet Modelle\n",
    "    if encoder_str == \"resnet50\":\n",
    "        encoder = models.resnet50(pretrained=pretrained, replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "    elif encoder_str == \"resnet101\":\n",
    "        encoder = models.resnet101(pretrained=pretrained, replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "    elif encoder_str == \"resnet152\":\n",
    "        encoder = models.resnet152(pretrained=pretrained, replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "    else:\n",
    "        raise f\"Not implemented encoder {encoder_str}\"\n",
    "\n",
    "    if in_channels != 3:\n",
    "        conv1 = encoder.conv1[0] if \"st\" in encoder else encoder.conv1\n",
    "        conv1 = torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            conv1.out_channels,\n",
    "            kernel_size=conv1.kernel_size,\n",
    "            stride=conv1.stride,\n",
    "            padding=conv1.padding,\n",
    "            bias=conv1.bias,\n",
    "        )\n",
    "        if \"st\" in encoder:\n",
    "            encoder.conv1[0] = conv1\n",
    "        else:\n",
    "            encoder.conv1 = conv1\n",
    "\n",
    "    encoder_layer1 = nn.Sequential(encoder.conv1, encoder.bn1, nn.ReLU(inplace=True))\n",
    "    encoder_layer2 = nn.Sequential(encoder.maxpool, encoder.layer1)\n",
    "    encoder_layer3 = encoder.layer2\n",
    "    encoder_layer4 = encoder.layer3\n",
    "    encoder_layer5 = encoder.layer4\n",
    "\n",
    "    return encoder_channels, encoder_layer1, encoder_layer2, encoder_layer3, encoder_layer4, encoder_layer5\n",
    "\n",
    "\n",
    "def get_decoder(encf, dilation, attn, no_skip=False, dec_interp=False):\n",
    "    decf = [512, 256, 128, 64, 32]\n",
    "    if dilation == 1:\n",
    "        decoder_layer1 = UpsampleBlock(encf[-1], decf[0], 0 if no_skip else encf[-2], attn, dec_interp)\n",
    "        decoder_layer2 = UpsampleBlock(decf[0], decf[1], 0 if no_skip else encf[-3], attn, dec_interp)\n",
    "        decoder_layer3 = UpsampleBlock(decf[1], decf[2], 0 if no_skip else encf[-4], attn, dec_interp)\n",
    "        decoder_layer4 = UpsampleBlock(decf[2], decf[3], 0 if no_skip else encf[-5], attn, dec_interp)\n",
    "        decoder_layer5 = UpsampleBlock(decf[3], decf[4], 0, attn, dec_interp)\n",
    "    elif dilation == 2:\n",
    "        decoder_layer1 = None\n",
    "        decoder_layer2 = UpsampleBlock(encf[-1], decf[1], 0 if no_skip else encf[-3], attn, dec_interp)\n",
    "        decoder_layer3 = UpsampleBlock(decf[1], decf[2], 0 if no_skip else encf[-4], attn, dec_interp)\n",
    "        decoder_layer4 = UpsampleBlock(decf[2], decf[3], 0 if no_skip else encf[-5], attn, dec_interp)\n",
    "        decoder_layer5 = UpsampleBlock(decf[3], decf[4], 0, attn, dec_interp)\n",
    "    elif dilation == 4:\n",
    "        decoder_layer1, decoder_layer2 = None, None\n",
    "        decoder_layer3 = UpsampleBlock(encf[-1], decf[2], 0 if no_skip else encf[-4], attn, dec_interp)\n",
    "        decoder_layer4 = UpsampleBlock(decf[2], decf[3], 0 if no_skip else encf[-5], attn, dec_interp)\n",
    "        decoder_layer5 = UpsampleBlock(decf[3], decf[4], 0, attn, dec_interp)\n",
    "    else:\n",
    "        raise ValueError(\"Dilation can be set to 1, 2 or 4\")\n",
    "    return decf, decoder_layer1, decoder_layer2, decoder_layer3, decoder_layer4, decoder_layer5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class UNetTemplate(nn.Module):\n",
    "    def __init__(self, args, in_channels=3):\n",
    "\n",
    "        super(UNetTemplate, self).__init__()\n",
    "        \n",
    "        self.use_ppm = args.ppm\n",
    "        self.use_aspp = args.aspp\n",
    "        self.dilation = args.dilation\n",
    "        self.no_skip = args.no_skip\n",
    "        self.interpolate = args.interpolate\n",
    "        self.enc_chn, self.enc_l1, self.enc_l2, self.enc_l3, self.enc_l4, self.enc_l5 = get_encoder(\n",
    "            args.encoder, self.dilation, in_channels=in_channels\n",
    "        )\n",
    "\n",
    "        if self.use_ppm:\n",
    "            self.ppm = PPM(self.enc_chn[-1])\n",
    "        elif self.use_aspp:\n",
    "            self.aspp = ASPP(self.enc_chn[-1], self.dilation)\n",
    "\n",
    "        self.dec_chn = None\n",
    "        if not self.interpolate:\n",
    "            self.dec_chn, self.dec_l1, self.dec_l2, self.dec_l3, self.dec_l4, self.dec_l5 = get_decoder(\n",
    "                self.enc_chn, self.dilation, args.attention, self.no_skip, args.dec_interp\n",
    "            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        enc1 = self.enc_l1(data)\n",
    "        enc2 = self.enc_l2(enc1)\n",
    "        enc3 = self.enc_l3(enc2)\n",
    "        enc4 = self.enc_l4(enc3)\n",
    "        enc5 = self.enc_l5(enc4)\n",
    "\n",
    "        if self.use_ppm:\n",
    "            enc5 = self.ppm(enc5)\n",
    "        elif self.use_aspp:\n",
    "            enc5 = self.aspp(enc5)\n",
    "        if self.interpolate:\n",
    "            return enc5, None, None\n",
    "\n",
    "        if self.dilation == 1:\n",
    "            if self.no_skip:\n",
    "                enc1, enc2, enc3, enc4 = None, None, None, None\n",
    "            dec1 = self.dec_l1(enc5, enc4)\n",
    "            dec2 = self.dec_l2(dec1, enc3)\n",
    "            dec3 = self.dec_l3(dec2, enc2)\n",
    "            dec4 = self.dec_l4(dec3, enc1)\n",
    "            dec5 = self.dec_l5(dec4, None)\n",
    "        elif self.dilation == 2:\n",
    "            if self.no_skip:\n",
    "                enc1, enc2, enc3 = None, None, None\n",
    "            dec2 = self.dec_l2(enc5, enc3)\n",
    "            dec3 = self.dec_l3(dec2, enc2)\n",
    "            dec4 = self.dec_l4(dec3, enc1)\n",
    "            dec5 = self.dec_l5(dec4, None)\n",
    "        elif self.dilation == 4:\n",
    "            if self.no_skip:\n",
    "                enc1, enc2 = None, None\n",
    "            dec3 = self.dec_l3(enc5, enc2)\n",
    "            dec4 = self.dec_l4(dec3, enc1)\n",
    "            dec5 = self.dec_l5(dec4, None)\n",
    "\n",
    "        return dec5, dec4, dec3\n",
    "\n",
    "# bekommt die Decoder Outputs als Eingabe - besteht aus einer Output-Blick Schicht, die die Schadensklasse für jedes Pixel vorhersagt\n",
    "\n",
    "class OutputTemplate(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_class: int, \n",
    "                 deep_supervision: bool, # Sollen beim Training Zwischen Schichten überwacht werden\n",
    "                 dec_chn: List[int], # Liste von Ints, die die Kanäle der Decoder-Schichten des Modells repräsentiert\n",
    "                 scale: int = 1, \n",
    "                 interp: bool = False, # interpolation der Ausgabe\n",
    "                 enc_last:int = 0): # Anzahl der Kanäle für die letzte Encoder schicht\n",
    "        \n",
    "        super(OutputTemplate, self).__init__()\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.interp = interp\n",
    "        if self.interp:\n",
    "            d5 = enc_last * scale\n",
    "            self.deep_supervision = False\n",
    "        else:\n",
    "            d3, d4, d5 = scale * dec_chn[-3], scale * dec_chn[-2], scale * dec_chn[-1]\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            self.output_block_ds3 = OutputBlock(d3, n_class, interp)\n",
    "            self.output_block_ds4 = OutputBlock(d4, n_class, interp)\n",
    "        self.output_block = OutputBlock(d5, n_class, interp)\n",
    "\n",
    "    def forward(self, dec5, dec4, dec3):\n",
    "        out = self.output_block(dec5)\n",
    "        if self.training and self.deep_supervision:\n",
    "            out_dec3 = self.output_block_ds3(dec3)\n",
    "            out_dec4 = self.output_block_ds4(dec4)\n",
    "            return [out, out_dec4, out_dec3]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concat(x, y):\n",
    "    return None if x is None or y is None else torch.cat([x, y], 1)\n",
    "\n",
    "class SiameseUNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 args: Any, \n",
    "                 n_classes: int ):\n",
    "        super(SiameseUNet, self).__init__()\n",
    "\n",
    "        # Parameter von args:\n",
    "        # args.ppm: bool              -> Aktiviert PPM (Pyramid Pooling Module)\n",
    "        # args.aspp: bool             -> Aktiviert ASPP (Atrous Spatial Pyramid Pooling)\n",
    "        # args.dilation: int          -> Dilationstyp (1, 2, 4)\n",
    "        # args.no_skip: bool          -> Deaktiviert Skip-Verbindungen\n",
    "        # args.interpolate: bool      -> Aktiviert Interpolation im Decoder\n",
    "        # args.deep_supervision: bool -> Aktiviert Tiefenüberwachung (falls gewünscht)\n",
    "        # args.encoder_str: str       -> kann resnet50, resnet101, resnet152 sein\n",
    "        \n",
    "    \n",
    "\n",
    "        # Übergabe der argumente and UNetTempale\n",
    "        self.unet = UNetTemplate(args)\n",
    "\n",
    "        # Übergabe der fest definierten n_class an OutputTemplate\n",
    "        self.output_block = OutputTemplate(\n",
    "            n_classes,\n",
    "            args.deep_supervision,\n",
    "            self.unet.dec_chn,\n",
    "            2,\n",
    "            args.interpolate,\n",
    "            self.unet.enc_chn[-1],\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Zugriff auf Pre- und Post-Bilder\n",
    "        pre_data = data[0]  # Vorher-Bild\n",
    "        post_data = data[1]  # Nachher-Bild\n",
    "        \n",
    "        # Verarbeitung der Bilder im UNet\n",
    "        pre_dec5, pre_dec4, pre_dec3 = self.unet(pre_data)  # Verarbeitung des Vorher-Bildes\n",
    "        post_dec5, post_dec4, post_dec3 = self.unet(post_data)  # Verarbeitung des Nachher-Bildes\n",
    "        \n",
    "        # Kombination der Ergebnisse von Pre- und Post-Bildern\n",
    "        dec5, dec4, dec3 = concat(pre_dec5, post_dec5), concat(pre_dec4, post_dec4), concat(pre_dec3, post_dec3)\n",
    "        \n",
    "        # Ausgabe des Modells\n",
    "        out = self.output_block(dec5, dec4, dec3)\n",
    "        return out\n",
    "\n",
    "\n",
    "    # def forward(self, data):\n",
    "        \n",
    "    #      # Vorwärtsdurchlauf: Verarbeitung von Pre- und Post-Disaster-Daten\n",
    "    #     pre_dec5, pre_dec4, pre_dec3 = self.unet(data[:, :3]) # da UNet5 Stufen hat - startet decoder bei 5 \n",
    "    #     post_dec5, post_dec4, post_dec3 = self.unet(data[:, 3:])\n",
    "\n",
    "    #     # Kombination der Ergebnisse von Pre- und Post-Bildern\n",
    "    #     dec5, dec4, dec3 = concat(pre_dec5, post_dec5), concat(pre_dec4, post_dec4), concat(pre_dec3, post_dec3)\n",
    "        \n",
    "    #     # Ausgabe des Modells\n",
    "    #     out = self.output_block(dec5, dec4, dec3)\n",
    "    #     return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguemnte die Übergeben werden müssen: \n",
    "\n",
    "n_classes = 5 Nummer der Klassen für Klassifikation\n",
    "        # Parameter von args:\n",
    "        # args.ppm: bool              -> Aktiviert PPM (Pyramid Pooling Module)\n",
    "        # args.aspp: bool             -> Aktiviert ASPP (Atrous Spatial Pyramid Pooling)\n",
    "        # args.dilation: int          -> Dilationstyp (1, 2, 4)\n",
    "        # args.no_skip: bool          -> Deaktiviert Skip-Verbindungen\n",
    "        # args.interpolate: bool      -> Aktiviert Interpolation im Decoder\n",
    "        # args.deep_supervision: bool -> Aktiviert Tiefenüberwachung (falls gewünscht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "# from utils.f1 import F1\n",
    "# from model.loss import Loss\n",
    "# from model.unet import SiameseUNet\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.f1_score = F1(args)\n",
    "        self.model = SiameseUNet(args, n_classes=5)\n",
    "        self.loss = Loss(args)\n",
    "        self.best_f1 = torch.tensor(0)\n",
    "        self.best_epoch = 0\n",
    "        self.tta_flips = [[2], [3], [2, 3]]\n",
    "        self.lr = args.lr\n",
    "        self.n_class = 5\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.test_idx = 0\n",
    "\n",
    "\n",
    "        ''' defines the computation performed at every call and must be overridden by all subclasses of torch.nn.Module. \n",
    "    takes input data, processes it through the networks layers and returns the output. \n",
    "    Output: logits, probabilities or any other form of processed data \n",
    "    '''\n",
    "\n",
    "    def forward(self, img):\n",
    "        pred = self.model(img)\n",
    "        if self.args.tta:\n",
    "            for flip_idx in self.tta_flips:\n",
    "                pred += self.flip(self.model(self.flip(img, flip_idx)), flip_idx)\n",
    "            pred /= len(self.tta_flips) + 1\n",
    "        return pred\n",
    "    \n",
    "    def compute_loss(self, preds, label):\n",
    "        if self.args.deep_supervision:\n",
    "            loss = self.loss(preds[0], label)\n",
    "            for i, pred in enumerate(preds[1:]):\n",
    "                downsampled_label = torch.nn.functional.interpolate(label.unsqueeze(1), pred.shape[2:])\n",
    "                loss += 0.5 ** (i + 1) * self.loss(pred, downsampled_label.squeeze(1))\n",
    "            c_norm = 1 / (2 - 2 ** (-len(preds)))\n",
    "            return c_norm * loss\n",
    "        return self.loss(preds, label)\n",
    "    \n",
    "    @staticmethod\n",
    "    def flip(data, axis):\n",
    "        return torch.flip(data, dims=axis)\n",
    "    \n",
    "    def save_predictions(self, preds, targets):\n",
    "        if self.args.loss_str == \"coral\":\n",
    "            probs = torch.sum(torch.sigmoid(preds) > 0.5, dim=1) + 1\n",
    "        elif self.args.loss_str == \"mse\":\n",
    "            probs = torch.round(F.relu(preds[:, 0], inplace=True)) + 1\n",
    "        else:\n",
    "            probs = self.softmax(preds)\n",
    "\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy().astype(np.uint8)\n",
    "        \n",
    "        for prob, target in zip(probs, targets):\n",
    "            fname = os.path.join(self.args.results, \"probs\", f\"test_damage_{self.test_idx:05d}\")\n",
    "            self.test_idx += 1\n",
    "            np.save(fname, prob)\n",
    "            Image.fromarray(target).save(fname.replace(\"probs\", \"targets\") + \"_target.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Argument Parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple, Dict, List, Any\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import PIL\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT = Path(r\"C:\\Users\\elena\\Documents\\04-geo-oma\\data\")\n",
    "# DATA_PATH = ROOT / \"xview2-subset\"\n",
    "# IMG_PATH = DATA_PATH / \"png_images\"\n",
    "# TARGET_PATH = DATA_PATH / \"targets\"\n",
    "\n",
    "# EXPERIMENT_GROUP = \"local1\"\n",
    "# EXPERIMENT_ID = \"exp_001\"\n",
    "# # Local dev client path config\n",
    "# # uncomment these lines and make adjustments if necessary\n",
    "# EXPERIMENT_DIR = ROOT / \"experiments\" / EXPERIMENT_GROUP\n",
    "# EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CHECKPOINTS_DIR = ROOT / \"checkpoints\" / EXPERIMENT_GROUP\n",
    "# CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# writer = SummaryWriter(EXPERIMENT_DIR / EXPERIMENT_ID)\n",
    "\n",
    "# TRAIN_BATCH_SIZE = 5\n",
    "# VAL_BATCH_SIZE = 5\n",
    "\n",
    "# VAL_SCORE_THRESHOLD =0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPC Terrabyte\n",
    "# adapt the user to your needs\n",
    "USER = \"di97ren\"\n",
    "# keep the following unchanged\n",
    "ROOT = Path(\"/dss/dsstbyfs02/pn49ci/pn49ci-dss-0022\")\n",
    "USER_PATH = ROOT / f\"users/{USER}\"\n",
    "DATA_PATH = ROOT / \"data\"\n",
    "\n",
    "# when you are on a local dev client\n",
    "# uncomment these lines and make necessary ajdustments\n",
    "#ROOT = Path(\"C:/projects/hands-on-DL\")\n",
    "#DATA_PATH = Path(\"../data\")\n",
    "\n",
    "# Configure the path to the GWHD dataset for your environment\n",
    "DATASET_ROOT = DATA_PATH / \"xview2-subset\"\n",
    "\n",
    "IMG_PATH = DATASET_ROOT / \"png_images\"\n",
    "TARGET_PATH = DATASET_ROOT / 'targets'\n",
    "\n",
    "EXPERIMENT_GROUP = \"XView2\"\n",
    "EXPERIMENT_ID = \"exp_001\"\n",
    "\n",
    "# HPC Terrabyte path config\n",
    "EXPERIMENT_DIR = USER_PATH / f\"experiments/{EXPERIMENT_GROUP}\"\n",
    "EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINTS_DIR = USER_PATH / f\"checkpoints/{EXPERIMENT_GROUP}\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Local dev client path config\n",
    "# uncomment these lines and make adjustments if necessary\n",
    "# EXPERIMENT_DIR = ROOT / \"experiments\" / EXPERIMENT_GROUP\n",
    "# EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CHECKPOINTS_DIR = ROOT / \"checkpoints\" / EXPERIMENT_GROUP\n",
    "# CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(EXPERIMENT_DIR / EXPERIMENT_ID)\n",
    "\n",
    "# Configure the batch size in order to fit the model into your GPU (these settings were used on a A100 GPU)\n",
    "TRAIN_BATCH_SIZE = 5\n",
    "VAL_BATCH_SIZE = 5\n",
    "\n",
    "# the val score threshold is used to determine if a prediction should be considered\n",
    "# 0.0 means that all predictions are considered, normally this is what you want\n",
    "VAL_SCORE_THRESHOLD = 0.0\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsstbyfs02/pn49ci/pn49ci-dss-0022/data/xview2-subset\n"
     ]
    }
   ],
   "source": [
    "print(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    # train_images_path = IMG_PATH,\n",
    "    # train_masks_path = TARGET_PATH,\n",
    "    # val_images_path = IMG_PATH,\n",
    "    # val_masks_path = TARGET_PATH,\n",
    "    # test_images_path = IMG_PATH,\n",
    "    # test_masks_path = TARGET_PATH,\n",
    "    # batch_size = TRAIN_BATCH_SIZE,\n",
    "    epochs = 10,\n",
    "    lr = 0.0001,\n",
    "    tta = True,\n",
    "    deep_supervision = False,\n",
    "    loss_str = 'mse',\n",
    "    results = EXPERIMENT_DIR,\n",
    "    optimizer = 'adamw', \n",
    "    weight_decay = 1e-5,\n",
    "    momentum = 0.9,\n",
    "    use_scheduler = False,\n",
    "    ppm = True,\n",
    "    aspp = False,\n",
    "    dilation = 1,\n",
    "    no_skip = True,\n",
    "    interpolate = True, \n",
    "    encoder = \"resnet50\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs found: 20\n",
      "Total pairs found: 20\n",
      "Total pairs found: 20\n"
     ]
    }
   ],
   "source": [
    "train_dataset = xView2Dataset(\n",
    "    png_path=IMG_PATH,\n",
    "    target_path = TARGET_PATH,\n",
    "    # transform = transform(),\n",
    "    # image_transform = image_transform()\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 3,\n",
    "    prefetch_factor = 2,\n",
    "    drop_last = True,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    " \n",
    "val_dataset = xView2Dataset(\n",
    "    png_path=IMG_PATH,\n",
    "    target_path = TARGET_PATH,\n",
    "    # transform = transform(),\n",
    "    # image_transform = image_transform()\n",
    "    )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = VAL_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 3,\n",
    "    prefetch_factor = 2,\n",
    "    drop_last = True,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "test_dataset = xView2Dataset(\n",
    "    png_path=IMG_PATH,\n",
    "    target_path = TARGET_PATH,\n",
    "    # transform = transform(),\n",
    "    # image_transform = image_transform()\n",
    "    )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 3,\n",
    "    prefetch_factor = 2,\n",
    "    drop_last = True,\n",
    "    collate_fn = collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "Image dimensions: torch.Size([10, 3, 1024, 1024])\n",
      "Mask dimensions: torch.Size([10, 1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "     # Beispiel, um Dimensionen des Dataloaders zu überprüfen\n",
    "for i, batch in enumerate(test_dataloader):  # Oder valid_loader für das Validierungsset\n",
    "    images = batch[\"image\"]\n",
    "    masks = batch[\"masks\"]\n",
    "    \n",
    "    # Zeige die Dimensionen von Bildern und Masken an\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"Image dimensions: {images.shape}\")  # Form der Bilder\n",
    "    print(f\"Mask dimensions: {masks.shape}\")    # Form der Masken\n",
    "    \n",
    "    # Stoppe nach dem ersten Batch (optional)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, train_loader, optimizer, device):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         # Hole Bilder und Masken\n",
    "#         img, lbl = batch[\"image\"].to(device), batch[\"masks\"].to(device)\n",
    "\n",
    "#         # Überprüfe die Dimensionen der Eingabedaten\n",
    "#         print(f\"Image dimensions: {img.shape}\")\n",
    "#         print(f\"Mask dimensions: {lbl.shape}\")\n",
    "\n",
    "#         # Sicherstellen, dass es sich um 4D-Tensoren handelt\n",
    "#         assert img.ndimension() == 4, f\"Expected 4D input (got {img.ndimension()}D input)\"\n",
    "#         assert lbl.ndimension() == 4, f\"Expected 4D input (got {lbl.ndimension()}D input)\"\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Berechne die Vorhersage\n",
    "#         pred = model(img)\n",
    "\n",
    "#         # Berechne den Verlust\n",
    "#         loss = model.compute_loss(pred, lbl)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Backpropagation und Optimierung\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device): # adding epoch ??\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        img = [x.to(device) for x in batch[\"image\"]]  # Beide Bilder zu einem Tensor mit 6 Kanälen\n",
    "        lbl = [x.to(device) for x in batch[\"masks\"]]\n",
    "\n",
    "    # for batch in train_loader:\n",
    "    #     img = [x.to(device) for x in batch[\"image\"]]\n",
    "    #     lbl = batch['mask'].to(device)\n",
    "\n",
    "        # img, lbl = batch[\"image\"].to(device), batch[\"masks\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model.model(img)  # Use model.model directly for training\n",
    "        loss = model.compute_loss(pred, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, device): # epoch?\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    model.f1_score.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            img = [x.to(device) for x in batch[\"image\"]]\n",
    "            lbl = [x.to(device) for x in batch[\"masks\"]]\n",
    "\n",
    "\n",
    "            # img, lbl = batch[\"image\"].to(device), batch[\"masks\"].to(device)\n",
    "            pred = model(img)  # Use model for validation (includes TTA if enabled)\n",
    "            loss = model.loss(pred, lbl)\n",
    "            model.f1_score.update(pred, lbl)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    f1_score, dmgs_f1 = model.f1_score.compute()\n",
    "    model.f1_score.reset()\n",
    "    \n",
    "    # Update best F1 score\n",
    "    if f1_score >= model.best_f1:\n",
    "        model.best_f1 = f1_score\n",
    "        model.best_epoch = current_epoch  # You need to keep track of current_epoch\n",
    "    \n",
    "    metrics = {\n",
    "        \"f1\": round(f1_score.item(), 3),\n",
    "        \"val_loss\": round(total_loss / len(val_loader), 3),\n",
    "        \"top_f1\": round(model.best_f1.item(), 3),\n",
    "    }\n",
    "    \n",
    "    # Add damage scores if available\n",
    "    if dmgs_f1 is not None:\n",
    "        for i in range(4):\n",
    "            metrics.update({f\"D{i+1}\": round(dmgs_f1[i].item(), 3)})\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model.f1_score.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            img = [x.to(device) for x in batch[\"image\"]]\n",
    "            lbl = [x.to(device) for x in batch[\"masks\"]]\n",
    "\n",
    "\n",
    "            # img, lbl = batch[\"image\"].to(device), batch[\"masks\"].to(device)\n",
    "            pred = model(img)\n",
    "            model.f1_score.update(pred, lbl)\n",
    "            model.save_predictions(pred, lbl)\n",
    "    \n",
    "    f1_score, dmgs_f1 = model.f1_score.compute()\n",
    "    model.f1_score.reset()\n",
    "    \n",
    "    metrics = {\"f1\": round(f1_score.item(), 3)}\n",
    "    if dmgs_f1 is not None:\n",
    "        for i in range(4):\n",
    "            metrics.update({f\"D{i+1}\": round(dmgs_f1[i].item(), 3)})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, args, device):\n",
    "    # Setup optimizer\n",
    "    if args.optimizer.lower() == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    elif args.optimizer.lower() == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    elif args.optimizer.lower() == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        # Add other optimizers as needed\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # Setup scheduler if needed\n",
    "    scheduler = None\n",
    "    if args.use_scheduler:\n",
    "        # Implement your own scheduler or use PyTorch's built-in schedulers\n",
    "        # For example, to mimic the NoamLR scheduler:\n",
    "        from torch.optim.lr_scheduler import LambdaLR\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            warmup_steps = args.warmup * len(train_loader)\n",
    "            total_steps = args.epochs * len(train_loader)\n",
    "            step += 1  # Avoid division by zero\n",
    "            \n",
    "            if step < warmup_steps:\n",
    "                return args.init_lr + step * (args.lr - args.init_lr) / warmup_steps\n",
    "            else:\n",
    "                return args.lr * (args.final_lr / args.lr) ** ((step - warmup_steps) / (total_steps - warmup_steps))\n",
    "        \n",
    "        scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        metrics = validate(model, val_loader, device, epoch)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={metrics['val_loss']:.4f}, f1={metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Update scheduler\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, LambdaLR):\n",
    "                for _ in range(len(train_loader)):\n",
    "                    scheduler.step()\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if metrics['f1'] >= model.best_f1:\n",
    "            print(f\"Saving best model with F1 score: {metrics['f1']:.4f}\")\n",
    "            torch.save(model.state_dict(), os.path.join(args.results, \"best_model.pth\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/08/di97ren/04-geo-oma24/assignment_04_geo_oma24-xView2/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/dss/dsshome1/08/di97ren/04-geo-oma24/assignment_04_geo_oma24-xView2/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m os.makedirs(os.path.join(args.results, \u001b[33m\"\u001b[39m\u001b[33mprobs\u001b[39m\u001b[33m\"\u001b[39m), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m os.makedirs(os.path.join(args.results, \u001b[33m\"\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m\"\u001b[39m), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m      3\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Create dataloaders (you need to implement this based on your data)\u001b[39;00m\n\u001b[32m      9\u001b[39m train_loader = train_dataloader\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.args = args\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.f1_score = F1(args)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSiameseUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.loss = Loss(args)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.best_f1 = torch.tensor(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mSiameseUNet.__init__\u001b[39m\u001b[34m(self, args, n_classes)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28msuper\u001b[39m(SiameseUNet, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Parameter von args:\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# args.ppm: bool              -> Aktiviert PPM (Pyramid Pooling Module)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# args.aspp: bool             -> Aktiviert ASPP (Atrous Spatial Pyramid Pooling)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Übergabe der argumente and UNetTempale\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.unet = \u001b[43mUNetTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Übergabe der fest definierten n_class an OutputTemplate\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.output_block = OutputTemplate(\n\u001b[32m     27\u001b[39m     n_classes,\n\u001b[32m     28\u001b[39m     args.deep_supervision,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     \u001b[38;5;28mself\u001b[39m.unet.enc_chn[-\u001b[32m1\u001b[39m],\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mUNetTemplate.__init__\u001b[39m\u001b[34m(self, args, in_channels)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.enc_chn, \u001b[38;5;28mself\u001b[39m.enc_l1, \u001b[38;5;28mself\u001b[39m.enc_l2, \u001b[38;5;28mself\u001b[39m.enc_l3, \u001b[38;5;28mself\u001b[39m.enc_l4, \u001b[38;5;28mself\u001b[39m.enc_l5 = get_encoder(\n\u001b[32m     79\u001b[39m     args.encoder, \u001b[38;5;28mself\u001b[39m.dilation, in_channels=in_channels\n\u001b[32m     80\u001b[39m )\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_ppm:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28mself\u001b[39m.ppm = \u001b[43mPPM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc_chn\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_aspp:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.aspp = ASPP(\u001b[38;5;28mself\u001b[39m.enc_chn[-\u001b[32m1\u001b[39m], \u001b[38;5;28mself\u001b[39m.dilation)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mPPM.__init__\u001b[39m\u001b[34m(self, in_channels)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.features = []\n\u001b[32m      5\u001b[39m out_channels = in_channels // \u001b[32m4\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43min_channels\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m())\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mbin\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m6\u001b[39m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.features.append(\n\u001b[32m      9\u001b[39m         nn.Sequential(\n\u001b[32m     10\u001b[39m             nn.AdaptiveAvgPool2d(\u001b[38;5;28mbin\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         )\n\u001b[32m     15\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Model(args).to(device)\n",
    "    \n",
    "    # Create dataloaders (you need to implement this based on your data)\n",
    "    train_loader = train_dataloader\n",
    "    val_loader = val_dataloader\n",
    "    test_loader = test_dataloader\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(model, train_loader, val_loader, args, device)\n",
    "    \n",
    "    # Load best model for testing\n",
    "    model.load_state_dict(torch.load(os.path.join(args.results, \"best_model.pth\")))\n",
    "    \n",
    "    # Test model\n",
    "    test_metrics = test(model, test_loader, device)\n",
    "    print(f\"Test metrics: {test_metrics}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parse arguments\n",
    "    args = args\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs(args.results, exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.results, \"probs\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.results, \"targets\"), exist_ok=True)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
